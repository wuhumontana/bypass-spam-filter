{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBdiD49e5Q7L"
   },
   "source": [
    "# **Adversarial Attacks Against Machine Learning-Based Spam Filters and Their Defense**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPJYb7EA5a24"
   },
   "source": [
    "Team Members: **Kyle Wang**, **Brian Zhou**, **Jim Huang**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSubQvScyKdQ"
   },
   "source": [
    "## **Introduction**\n",
    "Machine learning-based spam detection models learn from a set of labeled training data and detect spam emails using this trained model. In this capstone, we study a class of vulnerabilities of such detection models, where the attack can manipulate the numerical features used in such a model ( e.g., TF-IDF vectors representing emails to a SVM classifier) to misclassify them during the detection phase. However, very often feature extraction methods make it difficult to translate a change made to the features to that in the textual email space. This lab uses a new attack method of making guided changes to the text in emails by taking advantage of purposely generated adversarial TF-IDF vetor representing emails. We identify a set of \"magic words\", or malicious words, to be added to a spam email, which can cause desirable misclassifications by classifiers. This attack works in a similar way to the so-called \"good word attack\".\n",
    "\n",
    "For more information on this attack approach, you can refer to the following publications:\n",
    "\n",
    "(1) Q. Cheng, A. Xu, X. Li, and L. Ding, “Adversarial Email Generation against Spam Detection Models through Feature Perturbation,” The 2022 IEEE International Conference on Assured Autonomy (ICAA’22), Virtual Event, March 22-23, 2022. [Download](https://isi.jhu.edu/wp-content/uploads/2022/04/Adversarial_Attacks_Against_Machine_Learning_Based_SpamFilters__IEEE.pdf)\n",
    "\n",
    "(2) J. He, Q. Cheng, and X. Li, “Understanding the Impact of Bad Words \n",
    "on Email Management through Adversarial Machine Learning,” SIG-KM International Research Symposium 2021, Virtual Event, The University of North Texas, September 29, 2021. [Download](https://isi.jhu.edu/wp-content/uploads/2021/10/Bad-Words-He-Cheng-Li-Rev.pdf)\n",
    "\n",
    "(3) C. Wang, D. Zhang, S. Huang, X. Li, and L. Ding, “Crafting Adversarial Email Content against Machine Learning Based Spam Email Detection,” In Proceedings of the 2021 International Symposium on Advanced Security on Software and Systems (ASSS ’21) with AsiaCCS 2021, Virtual Event, Hong Kong, June 7, 2021. [Download](https://isi.jhu.edu/wp-content/uploads/2021/04/ASSS_Workshop_Paper.pdf\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sv-y6Ac6FYWO"
   },
   "source": [
    "## **1. Loading Dataset**\n",
    "We will be using the Ling-Spam. The Ling-Spam dataset is a collection of 2,893 spam and non-spam messages curated from Linguist List. The messages in the dataset revolve around linguistic interests, such as job postings, research opportunities and software discussion.\n",
    "\n",
    "### Acknowledgements\n",
    "The dataset and its information come from the original authors of \"A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists\". The dataset was made publicly available as a part of that paper. \\\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAHEUxXQcEqx"
   },
   "source": [
    "### Definition of each variables\n",
    "x_train: Training data features\n",
    "\n",
    "x_val  : Validation data features (This is used to find the magic words)\n",
    "\n",
    "x_test : Testing data features\n",
    "\n",
    "\\\\\n",
    "\n",
    "y-train: Training data label\n",
    "\n",
    "y_val  : Validation data label\n",
    "\n",
    "y_test : Testing data label\n",
    "\n",
    "**Run the code block below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_subdatasets(percentage):\n",
    "\n",
    "    df = pd.read_csv('./kaggle_1.csv', encoding='utf-8', encoding_errors='ignore')\n",
    "    # Define the number of rows in the original dataset\n",
    "    total_rows = len(df)\n",
    "    print(len(df))\n",
    "    \n",
    "    # Calculate the number of rows for each subset\n",
    "    subset1_size = 1000\n",
    "    subset2_size = 1000\n",
    "    \n",
    "    # Calculate the number of rows for the common subset (20% of the total rows)\n",
    "    common_size = int(percentage * subset1_size)\n",
    "    print(common_size)\n",
    "\n",
    "    # Randomly select rows for the common subset\n",
    "    common_indices = np.random.choice(total_rows, common_size, replace=False)\n",
    "    common_subset = df.iloc[common_indices]\n",
    "    \n",
    "    # Create a dataset without the rows already present in the common subset\n",
    "    different_subset = df.drop(common_indices)\n",
    "    \n",
    "    # Create the first subset with 4000 rows and 20% common rows\n",
    "    sub_dataset1_common = pd.concat([common_subset, different_subset.sample(subset1_size - common_size)])\n",
    "    \n",
    "    # Create the second subset with 12000 rows and 20% common rows\n",
    "    sub_dataset2_common = pd.concat([common_subset, different_subset.sample(subset2_size - common_size)])\n",
    "    \n",
    "    return sub_dataset1_common, sub_dataset2_common\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# # Assuming 'original_df' is your original dataset (DataFrame) with 16000 rows\n",
    "# # Call the function to generate two sub-datasets\n",
    "# df_shadow, df_target = create_subdatasets(0.5)\n",
    "\n",
    "# # Output the sizes of the resulting sub-datasets\n",
    "# print(f\"Size of df_shadow: {len(df_shadow)}\")\n",
    "# print(f\"Size of df_target: {len(df_target)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "CZvxGJk-rkkc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tkinter import YView\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# This function extracts data from .csv file and split into training, validation, and testing dataset.\n",
    "def data_extraction(df, valid = True):\n",
    "  # Separate data into features and labels\n",
    "  x = df.message\n",
    "  y = df.label\n",
    "\n",
    "  y = y.replace('ham', 0)\n",
    "  y = y.replace('spam', 1)\n",
    "  \n",
    "  x = x.fillna('')\n",
    "  y = y.fillna(0)\n",
    "  x = x.astype(str)\n",
    "  y = y.astype(int)\n",
    "\n",
    "  # We first separate the entire dataset to 80% and 20%\n",
    "  # We use the 90% to get our training dataset and the validation dataset. \n",
    "  # We use the 10% as our testing dataset. \n",
    "  x_train_val, x_test, y_train_val, y_test = train_test_split(x, y, test_size=0.1, random_state=99, stratify = y)\n",
    "\n",
    "  # Separate the 80%, which contains our traning dataset and validation dataset, into another 80% traning dataset and 20% valications dataset.\n",
    "  if valid:\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.2, random_state=99, stratify=y_train_val)\n",
    "  else:\n",
    "    x_train, y_train = x_train_val, y_train_val\n",
    "\n",
    "  return x_train, x_val, x_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def copy_dataset_by_similarity(x_train, y_train, similarity = 0.2):\n",
    "  # Reset the index to ensure default integer indexing\n",
    "  x_train = x_train.reset_index(drop=True)\n",
    "  y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "  # Calculate the shadow set size as 10% of the training set\n",
    "  shadow_size = int(similarity * x_train.shape[0])\n",
    "\n",
    "  # Create the shadow sets without using indices directly\n",
    "  x_train_shadow = x_train.sample(n=shadow_size, random_state=99)\n",
    "  y_train_shadow = y_train.loc[x_train_shadow.index]\n",
    "\n",
    "  return x_train_shadow, y_train_shadow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFUv7AmsFW8e"
   },
   "source": [
    "In the code block above, we have read the dataset into variables 'messages' and 'labels'. Variable 'messages' contains the email messages and variable 'labels' contains the class labels where 0 represents ham and 1 represents spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v399Bk1sjKcA"
   },
   "source": [
    "We split the entire dataset into three different subsets: the training data, the validation data, and the testing data. \n",
    "\n",
    "In the code block above, we split the dataset twice using a 64:16:20 ratio, where 64% of the entire dataset is assigned to the training dataset (Y_train), 16% to the validation dataset (X_val), and 20% to the testing dataset (X_test), ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cn-R5V0HlFYm"
   },
   "source": [
    "### **Additional Information of Three Datasets**\n",
    "In the above operation, we divided the entire dataset into three parts: training dataset, validation dataset, and testing dataset. This is done to evaluate the performance of our machine learning model on new and unseen data. The training dataset is used to train the model, the validation dataset is used to tune the model's hyperparameters(magic words in our application), and the testing dataset is used to evaluate the final performance of the model. \n",
    "\n",
    "For more information on these concepts, you can read the article available at the following link: https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KJ7iGauJih3"
   },
   "source": [
    "## **2. Preprocessing the Emails**\n",
    "To extract only useful information from the emails we used, we applied serveral data preprocessing steps.\n",
    "\n",
    "(1). We removed all HTML tags, numbers, punctuation marks, and English stop words. \n",
    "\n",
    "(2). We converted all words to their lowercase forms and combined each paragraph into a single line instead of multiple lines. \n",
    "\n",
    "(3). We conducted stemming on all the remaining words to reduce them to their root forms. \\\\\n",
    "\n",
    "**Run the code below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZfS6VpTaH7Wu",
    "outputId": "fc7a0c4c-9572-43ac-f1fc-61bc27748fd0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yixuan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/yixuan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/yixuan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Download required packages from nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Clean up the text data.\n",
    "def remove_hyperlink(word):\n",
    "    return re.sub(r\"http\\S+\", \" \", word)\n",
    "\n",
    "# Convert the letter to lowercase.\n",
    "def to_lower(word):\n",
    "    result = word.lower()\n",
    "    return result\n",
    "\n",
    "# Remove the numbers.\n",
    "def remove_number(word):\n",
    "    result = re.sub(r'\\d+', ' ', word)\n",
    "    return result\n",
    "\n",
    "# Remove the puncturations.\n",
    "def remove_punctuation(word):\n",
    "    result = word.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
    "    return result\n",
    "\n",
    "# Remove the whitespace. \n",
    "def remove_whitespace(word):\n",
    "    result = word.strip()\n",
    "    return result\n",
    "\n",
    "# Merge multiple lines into one line.\n",
    "def replace_newline(word):\n",
    "    return word.replace('\\n', ' ')\n",
    "\n",
    "\n",
    "def clean_up_pipeline(sentence):\n",
    "    cleaning_utils = [remove_hyperlink,replace_newline,to_lower, remove_number, remove_punctuation, remove_whitespace]\n",
    "    for o in cleaning_utils:\n",
    "        sentence = o(sentence)\n",
    "    return sentence\n",
    "\n",
    "# Remove the stopwords, for example: a, and, an, above, ..., etc.\n",
    "def remove_stop_words(words):\n",
    "    result = [i for i in words if i not in ENGLISH_STOP_WORDS]\n",
    "    return result\n",
    "\n",
    "# Stemming aims to cut off the ends of words to obtain the root form. \n",
    "# It does not consider the meaning of the word. As a result, the stemmed word might not be a valid word in the language.\n",
    "# Generally faster than lemmatization because it's based on rules and doesn't require additional resources like a lexicon.\n",
    "# Example: \"running\" → \"run\", \"flies\" → \"fli\", \"happily\" → \"happili\"\n",
    "def word_stemmer(words):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(o) for o in words]\n",
    "\n",
    "# Lemmatization reduces words to their base or dictionary form by considering the meaning and context of the word. \n",
    "# It ensures that the root word (lemma) is a valid word in the language.\n",
    "# Generally more accurate than stemming but can be slower because it involves looking up words in a lexicon.\n",
    "# Example: \"running\" → \"run\", \"flies\" → \"fly\", \"better\" → \"good\"\n",
    "def word_lemmatizer(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(o) for o in words]\n",
    "\n",
    "# Clear out the unnecessary information.\n",
    "def clean_token_pipeline(words):\n",
    "    cleaning_utils = [remove_stop_words, word_lemmatizer]\n",
    "    for o in cleaning_utils:\n",
    "        words = o(words)\n",
    "    return words\n",
    "\n",
    "# Preprocess the text data.\n",
    "def preprocess(X_train, X_val, X_test):\n",
    "    print(f'Remove hyperlink...')\n",
    "    print(f'Merge into one line...')\n",
    "    print(f'Convert to lowercase...')\n",
    "    print(f'Remove number...')\n",
    "    print(f'Remove punctuation...')\n",
    "    print(f'Remove whitespace...')\n",
    "    x_train = [clean_up_pipeline(o) for o in X_train]\n",
    "    x_val = [clean_up_pipeline(o) for o in X_val]\n",
    "    x_test = [clean_up_pipeline(o) for o in X_test]\n",
    "    \n",
    "    print(f'Word Tokenize...')\n",
    "    # word_tokenize() function is used to split a sentence or paragraph into individual words.\n",
    "    x_train = [word_tokenize(o) for o in x_train]\n",
    "    x_val = [word_tokenize(o) for o in x_val]\n",
    "    x_test = [word_tokenize(o) for o in x_test]\n",
    "\n",
    "    print(f'Remove stop words...')\n",
    "    print(f'Lemmatize...')\n",
    "    x_train = [clean_token_pipeline(o) for o in x_train]\n",
    "    x_val = [clean_token_pipeline(o) for o in x_val]\n",
    "    x_test = [clean_token_pipeline(o) for o in x_test]\n",
    "\n",
    "    x_train = [\" \".join(o) for o in x_train]\n",
    "    x_val = [\" \".join(o) for o in x_val]\n",
    "    x_test = [\" \".join(o) for o in x_test]    \n",
    "\n",
    "    return x_train, x_val, x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N68YIZqIJmc8"
   },
   "source": [
    "## **3. Feature Extraction**\n",
    "In this step, we aim to transform the text content of an email into a numerical feature vector that captures the essential information used for classification. To achieve this, we can choose from a variety of vectorization techniques(TF-IDF and a modified Word2vec) that convert text data into numerical vectors. \n",
    "\n",
    "**Run the code block below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wlpwcsgxJrMK",
    "outputId": "60937c8d-2514-4de8-f63b-0f53f4eb82c0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/yixuan/opt/anaconda3/lib/python3.9/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/yixuan/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/yixuan/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.9.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/yixuan/opt/anaconda3/lib/python3.9/site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "!pip install --upgrade gensim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def TfidfConvert(x_train, x_test, x_val, vectorizer):\n",
    "\n",
    "    raw_sentences = x_train + x_val\n",
    "    \n",
    "    # Learn vocabulary and compute IDF values. This vocabulary will be used later to convert new sentences into vectors.\n",
    "    # Calculate IDF values for each word, store this information for later use during the transformation of text into vectors.\n",
    "    # IDF(Inverse Document Frequency) is a measure of how important a word is to the entire corpus. \n",
    "    # Words that are frequent in a specific document but rare across the entire dataset will have a high IDF weight, vice versa.\n",
    "    vectorizer.fit(raw_sentences)\n",
    "    \n",
    "    # vectorizer.transform() is used to convert raw text data into a numerical format suitable for ML algorithms. \n",
    "    # Calculate the frequency of each word，which is the term frequency (TF) part of the TF-IDF. \n",
    "    # Calculate the final TF-IDF weight for a word in a specific document, which is the product of its TF and IDF values. \n",
    "    x_train_features = vectorizer.transform(x_train)\n",
    "    x_test_features = vectorizer.transform(x_test)\n",
    "    x_val_features = vectorizer.transform(x_val)\n",
    "\n",
    "    return x_train_features, x_test_features, x_val_features\n",
    "\n",
    "\n",
    "def getUniqueWords(allWords):\n",
    "    uniqueWords = []\n",
    "    for i in allWords:\n",
    "        if i not in uniqueWords:\n",
    "            uniqueWords.append(i)\n",
    "    return uniqueWords\n",
    "\n",
    "\n",
    "def input_split(x):\n",
    "    new_x = []\n",
    "    for line in x:\n",
    "        newline = line.split(' ')\n",
    "        new_x.append(newline)\n",
    "    return new_x\n",
    "\n",
    "\n",
    "def getUniqueWords(allWords):\n",
    "    uniqueWords = []\n",
    "    for i in allWords:\n",
    "        if i not in uniqueWords:\n",
    "            uniqueWords.append(i)\n",
    "    return uniqueWords\n",
    "\n",
    "\n",
    "def x2vec(input_x, feature_names, model):\n",
    "    x_features = []\n",
    "    for index in input_x:\n",
    "        model_vector = [0] * len(feature_names)\n",
    "\n",
    "        for token in index:\n",
    "            if token in feature_names:\n",
    "                feature_index = feature_names.index(token)\n",
    "\n",
    "                if model.wv.has_index_for(token):\n",
    "                    token_vecs = model.wv.get_vector(token)\n",
    "                    model_vector[feature_index] = token_vecs[0]\n",
    "        x_features.append(model_vector)\n",
    "    return x_features\n",
    "\n",
    "\n",
    "def single_transform(x, method, feature_model, feature_names, scaler, selection_model):\n",
    "    if method == 'tfidf':\n",
    "        result = feature_model.transform(x)\n",
    "\n",
    "        if selection_model != 'NaN':\n",
    "            result = selection_model.transform(result)\n",
    "        return result\n",
    "    else:\n",
    "        temp_x = x.values\n",
    "        temp_x = temp_x[0].split(' ')\n",
    "        model_vector = [0] * len(feature_names)\n",
    "        for token in temp_x:\n",
    "            if token in feature_names:\n",
    "                feature_index = feature_names.index(token)\n",
    "                if feature_model.wv.has_index_for(token):\n",
    "                    token_vecs = feature_model.wv.get_vector(token)\n",
    "                    model_vector[feature_index] = token_vecs[0]\n",
    "        x_features = [model_vector]\n",
    "        # x_features = np.array(x_features)\n",
    "        x_features = scaler.transform(x_features)\n",
    "        x_train_features = sparse.csr_matrix(x_features)\n",
    "        if selection_model != 'NaN':\n",
    "            x_train_features = selection_model.transform(x_train_features)\n",
    "        return x_train_features\n",
    "\n",
    "\n",
    "def feature_extraction(x_train, x_test, x_val, method):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    if method == 'tfidf':\n",
    "        x_train_features, x_test_features, x_val_features = TfidfConvert(x_train, x_test, x_val, vectorizer)\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "        return x_train_features, x_test_features, x_val_features, feature_names, vectorizer, 'NaN'\n",
    "\n",
    "    if method == 'word2vec':\n",
    "        temp_x_train = input_split(x_train)\n",
    "        temp_x_test = input_split(x_test)\n",
    "        temp_x_val = input_split(x_val)\n",
    "\n",
    "        model_train = Word2Vec(temp_x_train, vector_size=1)\n",
    "        feature_space = []\n",
    "        for index in temp_x_train:\n",
    "            feature_space = feature_space + getUniqueWords(index)\n",
    "        feature_names = getUniqueWords(feature_space)\n",
    "      \n",
    "        x_train_features = x2vec(temp_x_train, feature_names, model_train)\n",
    "        x_test_features = x2vec(temp_x_test, feature_names, model_train)\n",
    "        x_val_features = x2vec(temp_x_val, feature_names, model_train)\n",
    "\n",
    "        x_train_features = np.array(x_train_features)\n",
    "        x_test_features = np.array(x_test_features)\n",
    "        x_val_features = np.array(x_val_features)\n",
    "\n",
    "        pd.DataFrame(x_train_features).to_csv(\"x_train_features.csv\", header=None, index=False)\n",
    "        pd.DataFrame(x_test_features).to_csv(\"x_test_features.csv\", header=None, index=False)\n",
    "        pd.DataFrame(x_val_features).to_csv(\"x_val_features.csv\", header=None, index=False)\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(x_train_features)\n",
    "        x_train_features = scaler.transform(x_train_features)\n",
    "        x_test_features = scaler.transform(x_test_features)\n",
    "        x_val_features = scaler.transform(x_val_features)\n",
    "\n",
    "        x_train_features = sparse.csr_matrix(x_train_features)\n",
    "        x_test_features = sparse.csr_matrix(x_test_features)\n",
    "        x_val_features = sparse.csr_matrix(x_val_features)\n",
    "\n",
    "        return x_train_features, x_test_features, x_val_features, feature_names, model_train, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0RZeCBFLSH9"
   },
   "source": [
    "## **4. Training Classifiers**\n",
    "In this section, we will train a Support Vector Machine (SVM) or Logic Regression (LR) as an spam filter.\n",
    "\n",
    "**Run the code block below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kAeBdHNfLM7k",
    "outputId": "0e3dc5b4-535c-4e44-d5b3-9d5e4438008e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: secml in /Users/yixuan/opt/anaconda3/lib/python3.9/site-packages (0.15.6)\n",
      "Requirement already satisfied: python-dateutil in /Users/yixuan/opt/anaconda3/lib/python3.9/site-packages (from secml) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/yixuan/opt/anaconda3/lib/python3.9/site-packages (from secml) (1.21.5)\n",
      "Requirement already satisfied: Pillow>=6.2.1 in /Users/yixuan/opt/anaconda3/lib/python3.9/site-packages (from secml) (9.2.0)\n",
      "Requirement already satisfied: requests in /Users/yixuan/opt/anaconda3/lib/python3.9/site-packages (from secml) (2.28.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/yixuan/opt/anaconda3/lib/python3.9/site-packages (from secml) (1.9.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /Users/yixuan/opt/anaconda3/lib/python3.9/site-packages (from secml) (1.0.2)\n",
      "Requirement already satisfied: matplotlib>=3 in /Users/yixuan/opt/anaconda3/lib/python3.9/site-packages (from secml) (3.5.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /Users/yixuan/opt/anaconda3/lib/python3.9/site-packages (from secml) (1.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/yixuan/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3->secml) (1.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/yixuan/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3->secml) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/yixuan/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3->secml) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/yixuan/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3->secml) (4.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/yixuan/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3->secml) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/yixuan/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil->secml) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/yixuan/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.22->secml) (2.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yixuan/opt/anaconda3/lib/python3.9/site-packages (from requests->secml) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/yixuan/opt/anaconda3/lib/python3.9/site-packages (from requests->secml) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/yixuan/opt/anaconda3/lib/python3.9/site-packages (from requests->secml) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yixuan/opt/anaconda3/lib/python3.9/site-packages (from requests->secml) (2022.9.24)\n"
     ]
    }
   ],
   "source": [
    "!pip install secml\n",
    "from secml.data import CDataset\n",
    "from secml.data.splitter import CDataSplitterKFold\n",
    "from secml.ml.classifiers import CClassifierSVM\n",
    "from secml.ml.peval.metrics import CMetricAccuracy\n",
    "from secml.ml.peval.metrics import CMetricConfusionMatrix\n",
    "from secml.adv.attacks.evasion import CAttackEvasionPGD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from Feature_extraction import single_transform\n",
    "import csv\n",
    "from statistics import mean, stdev\n",
    "import threading\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    \"\"\"\n",
    "    classes = ['Negative', 'Positive']\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=cmap, xticklabels=classes, yticklabels=classes)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def print_cm(cm):\n",
    "    print(f'[TN   FP]')\n",
    "    print(f'[FN   TP]')\n",
    "    TN = cm[0, 0].item()\n",
    "    FP = cm[0, 1].item()\n",
    "    FN = cm[1, 0].item()\n",
    "    TP = cm[1, 1].item()\n",
    "\n",
    "    print(\"True Negatives (TN):\", TN)\n",
    "    print(\"False Positives (FP):\", FP)\n",
    "    print(\"False Negatives (FN):\", FN)\n",
    "    print(\"True Positives (TP):\", TP)\n",
    "\n",
    "    FNR = FN / (FN + TP)\n",
    "    print(\"False Negative Rate (FNR = FN / (FN + TP)): {:.2%}\".format(FNR))\n",
    "\n",
    "def train_SVM(x_train_features, x_test_features, y_train, y_test):\n",
    "    print(\"Start training SVM:\")\n",
    "    \n",
    "    # Create a dataset object that combines data and label so that they're organized and ready for SVM to learn form.\n",
    "    tr_set = CDataset(x_train_features, y_train)\n",
    "    test_set = CDataset(x_test_features, y_test)\n",
    "    \n",
    "    # Divide data into \"folds\" or chunks for cross-validation\n",
    "    # It is conducive for training and testing the classifier on different subsets of the data.\n",
    "    xval_splitter = CDataSplitterKFold()\n",
    "    \n",
    "    # Setting up an SVM classifier\n",
    "    clf_lin = CClassifierSVM()\n",
    "    \n",
    "    # 'C' controls the trade-off between getting as many training points correctly classified as possible \n",
    "    # and finding the widest possible decision margin (or the gap between data points from different classes).\n",
    "    xval_lin_params = {'C': [0.1, 1, 10, 100]}\n",
    "    \n",
    "    # Try out the 'C' on different parts of data to see which gives the best accuracy.\n",
    "    best_lin_params = clf_lin.estimate_parameters( \n",
    "        dataset=tr_set,\n",
    "        parameters=xval_lin_params,\n",
    "        splitter=xval_splitter,\n",
    "        metric='accuracy',\n",
    "        perf_evaluator='xval'   # adopt Cross-Validation\n",
    "    )\n",
    "    \n",
    "    print(\"The best training parameters are: \", \n",
    "          [(k, best_lin_params[k]) for k in sorted(best_lin_params)])\n",
    "    \n",
    "    # Use the best parameterer to train the classifiler\n",
    "    clf_lin.fit(tr_set.X, tr_set.Y)\n",
    "\n",
    "    # Use trained classifier to predict the validation data, calculate the accuracy of the valadation data\n",
    "    y_pred = clf_lin.predict(test_set.X)\n",
    "    metric = CMetricAccuracy()\n",
    "    acc = metric.performance_score(y_true=test_set.Y, y_pred=y_pred)\n",
    "    print(f'Validation Accuracy: {acc * 100}%')\n",
    "    \n",
    "    # Calculate and plot the confusion matrix of validation data\n",
    "    confusion_matrix = CMetricConfusionMatrix()\n",
    "    cm = confusion_matrix.performance_score(y_true=test_set.Y, y_pred=y_pred)\n",
    "    plot_confusion_matrix(np.array(cm.tondarray()))\n",
    "\n",
    "    print_cm(cm)\n",
    "    return tr_set, clf_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from secml.data import CDataset\n",
    "from secml.ml.classifiers import CClassifierLogistic\n",
    "from secml.ml.features import CNormalizerMinMax\n",
    "from secml.ml.peval.metrics import CMetricAccuracy\n",
    "from secml.array import CArray\n",
    "\n",
    "def train_LR(x_train_features, x_test_features, x_val_features, y_train, y_test, y_val):\n",
    "    print(\"Start training Logistic Regression:\")\n",
    "    \n",
    "    # Create a dataset object that combines data and label so that they're organized and ready for Logistic Regression to learn from.\n",
    "    tr_set = CDataset(x_train_features, y_train)\n",
    "    v_set = CDataset(x_val_features, y_val)\n",
    "    test_set = CDataset(x_test_features, y_test)\n",
    "\n",
    "    # Setting up a Logistic Regression classifier\n",
    "    lr_classifier = CClassifierLogistic()\n",
    "\n",
    "    # Train the classifier\n",
    "    lr_classifier.fit(tr_set.X, tr_set.Y)\n",
    "\n",
    "    # Use trained classifier to predict the validation data, calculate the accuracy of the validation data\n",
    "    y_pred = lr_classifier.predict(test_set.X)\n",
    "    metric = CMetricAccuracy()\n",
    "    acc = metric.performance_score(y_true=test_set.Y, y_pred=y_pred)\n",
    "    print(f'Validation Accuracy: {acc * 100}%')\n",
    "    \n",
    "    # Calculate and plot the confusion matrix of validation data\n",
    "    confusion_matrix = CMetricConfusionMatrix()\n",
    "    cm = confusion_matrix.performance_score(y_true=test_set.Y, y_pred=y_pred)\n",
    "    plot_confusion_matrix(np.array(cm.tondarray()))\n",
    "    print_cm(cm)\n",
    "\n",
    "    return tr_set, v_set, lr_classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rKjzZTCO5N2"
   },
   "source": [
    "## **5. PGD Attack**\n",
    "Our approach is based on successful adversarial perturbations made to model input features. We employ the Projected Gradient Descent (PGD) method to modify numeric feature values in the feature domain. PGD algorithm iteratively finds the needed changes with a constraint, *dmax*, which is the Euclidean distance to the original features indicating the allowed level of perturbations, to achieve the maximum loss in classification. In our approach, we run PGD over a set of spam emails and generate adversarial examples. Then we test these modified feature vectors to see whether they could successfully bypass the detection (i.e., being classified as ham).\n",
    "\n",
    "**Run the code block below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "lCvPfWA-P1OG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pgd_attack(clf_lin, tr_set, v_set, y_val, feature_names, nb_attack, dmax, lb, ub):\n",
    "\n",
    "    class_to_attack = 1\n",
    "    cnt = 0  # the number of success adversaril examples\n",
    "\n",
    "    ori_examples2_x = []\n",
    "    ori_examples2_y = []\n",
    "\n",
    "    # Randomly select samples from the validation dataset for the attack.\n",
    "    # The reason why choose validation dataset:\n",
    "    #  - Test the model's ability to handle adversarial inputs that are as realistic as possible, \n",
    "    #  - mimicking the behavior it would need to handle in the real world.\n",
    "    for i in range(nb_attack):\n",
    "        # looking for indices of elements where y_val is equal to class_to_attack.\n",
    "        idx_candidates = np.where(y_val == class_to_attack)\n",
    "        # Randomly select a samples\n",
    "        rn = np.random.choice(idx_candidates[0].size, 1)\n",
    "        x0, y0 = v_set[idx_candidates[0][rn[0]], :].X, v_set[idx_candidates[0][rn[0]], :].Y\n",
    "\n",
    "        x0 = x0.astype(float)\n",
    "        y0 = y0.astype(int)\n",
    "        x2 = x0.tondarray()[0]\n",
    "        y2 = y0.tondarray()[0]\n",
    "\n",
    "        ori_examples2_x.append(x2)\n",
    "        ori_examples2_y.append(y2)\n",
    "\n",
    "    # Setting parameters for PGD attack\n",
    "    solver_params = {\n",
    "        'eta': 0.01,\n",
    "        'max_iter': 1000,\n",
    "        'eps': 1e-4}\n",
    "    \n",
    "    # Type of perturbation 'l1' or 'l2'\n",
    "    noise_type = 'l2'  \n",
    "    y_target = 0\n",
    "    \n",
    "    # Initialize the PGD attack\n",
    "    pgd_attack = CAttackEvasionPGD(\n",
    "        classifier=clf_lin,\n",
    "        double_init_ds=tr_set,\n",
    "        distance=noise_type,\n",
    "        dmax=dmax,\n",
    "        lb=lb, ub=ub,\n",
    "        solver_params=solver_params,\n",
    "        y_target=y_target\n",
    "    )\n",
    "\n",
    "    ad_examples_x = []    # adversarial examples\n",
    "    ad_examples_y = []    # predicted labels\n",
    "    ad_index = []         # indices of successful adversarial attacks\n",
    "    cnt = 0               # the number of successful adversarial examples\n",
    "    for i in range(len(ori_examples2_x)):\n",
    "        x0 = ori_examples2_x[i]\n",
    "        y0 = ori_examples2_y[i]\n",
    "        \n",
    "        # y_pred_pgd  - The predicted label of the adversarial example.\n",
    "        # adv_ds_pgd  - The adversarially perturbed data generated by the PGD attack\n",
    "        y_pred_pgd, _, adv_ds_pgd, _ = pgd_attack.run(x0, y0)\n",
    "        \n",
    "        # Check Attack Success\n",
    "        if y_pred_pgd.item() == 0:\n",
    "            cnt = cnt + 1\n",
    "            ad_index.append(i)\n",
    "\n",
    "        ad_examples_x.append(adv_ds_pgd.X.tondarray()[0])\n",
    "        ad_examples_y.append(y_pred_pgd.item())\n",
    "\n",
    "        #attack_pt = adv_ds_pgd.X.tondarray()[0]\n",
    "\n",
    "    print('Success rate of PGD(validation): {:.2f}%'.format((cnt / nb_attack) * 100))\n",
    "\n",
    "\n",
    "    startTime2 = time.time()\n",
    "    \n",
    "    # Convert Lists to Arrays\n",
    "    ori_examples2_x = np.array(ori_examples2_x)\n",
    "    ori_examples2_y = np.array(ori_examples2_y)\n",
    "    ad_examples_x = np.array(ad_examples_x)\n",
    "    ad_examples_y = np.array(ad_examples_y)\n",
    "     \n",
    "    # Creates pandas DataFrames from the NumPy arrays of original and adversarial examples\n",
    "    ori_dataframe = pd.DataFrame(ori_examples2_x, columns=feature_names)\n",
    "    ad_dataframe = pd.DataFrame(ad_examples_x, columns=feature_names)\n",
    "\n",
    "    # Adds the adversarial labels\n",
    "    ad_dataframe['ad_label'] = ad_examples_y\n",
    "    # Separate out the successful and failed adversarial attacks \n",
    "    ad_success = ad_dataframe.loc[ad_dataframe.ad_label == 0]\n",
    "    ori_success = ori_dataframe.loc[ad_dataframe.ad_label == 0]\n",
    "    ad_fail = ad_dataframe.loc[ad_dataframe.ad_label == 1]\n",
    "    ori_fail = ori_dataframe.loc[ad_dataframe.ad_label == 1]\n",
    "\n",
    "    ad_success_x = ad_success.drop(columns=['ad_label'])\n",
    "    ad_fail_x = ad_fail.drop(columns=['ad_label'])\n",
    "\n",
    "    result = (ad_success_x - ori_success)\n",
    "    ori_dataframe.to_csv('ori_dataframe.csv')\n",
    "    ad_dataframe.to_csv('ad_dataframe.csv')\n",
    "    result.to_csv('result.csv')\n",
    "    \n",
    "    return result, cnt, ad_success_x, ori_dataframe, ori_examples2_y, cnt/nb_attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xUx6FBQiCBr"
   },
   "source": [
    "## **6. Magical Words**\n",
    "Adversarial emails are crafted by adding “magic words” to the original spam emails. The “magic words” are identified by intersecting the unique ham words with the “top words” identified during the adversarial perturbations. Specifically, the unique ham words are the words that only appear in ham emails but not in spam emails. After the PGD attack on the set of spam emails, we find which features are modified to the largest extent to bypass the detection. We then select a list of “top words” whose feature values have been changed the most. (The changes are measured by the variance of differences before and after the PGD perturbation.) In our experiments, we use the top 100 words, which is efficient. This set is relatively small and demonstrates a high success rate with the resulting magic words to fool the classifier. \n",
    "\n",
    "**Run the code block below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "Mc552xOTSQZn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def magical_word(x_train, x_val, x_test, y_train, y_val, y_test, result, cnt):\n",
    "    x2result1 = result\n",
    "    x2result1 = np.array(x2result1)\n",
    "    x2result = result\n",
    "    x2result = x2result.multiply(x2result1)\n",
    "    \n",
    "    # Save the top 100 features to the x2result.csv\n",
    "    sum_number = x2result.sum() / cnt\n",
    "    sum_number = pd.DataFrame(sum_number, columns=['sum_number'])\n",
    "    sum_number = sum_number.sort_values(\n",
    "        by='sum_number', ascending=False, inplace=False)\n",
    "    sum_number_pd = pd.DataFrame(sum_number.index[:100])\n",
    "    sum_number_pd.to_csv(\"x2result.csv\")\n",
    "    \n",
    "    # combine training and validation messages\n",
    "    d = {'message': x_train, 'label': y_train}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    d1 = {'message': x_val, 'label': y_val}\n",
    "    df1 = pd.DataFrame(data=d1)\n",
    "    frames = [df, df1]\n",
    "    messages = pd.concat(frames)\n",
    "    messages.to_csv(\"messages_train_val.csv\")\n",
    "    spam = messages[messages.label == 1]\n",
    "    ham = messages[messages.label == 0]\n",
    "    \n",
    "    d2 = {'message': x_test, 'label': y_test}\n",
    "    df2 = pd.DataFrame(data=d2)\n",
    "    frames2 = [df2]\n",
    "    messages_test = pd.concat(frames2)\n",
    "    messages_test.to_csv(\"messages_test.csv\")\n",
    "    spam_test = messages_test[messages_test.label == 1]\n",
    "\n",
    "    # Tf-idf for spam datasets\n",
    "    vect_spam = TfidfVectorizer()\n",
    "    vect_spam.fit_transform(spam['message'])\n",
    "    header_spam = vect_spam.get_feature_names_out()\n",
    "\n",
    "    # Tf-idf for ham datasets\n",
    "    vect_ham = TfidfVectorizer()\n",
    "    vect_ham.fit_transform(ham['message'])\n",
    "    header_ham = vect_ham.get_feature_names_out()\n",
    "\n",
    "    # find unique ham words\n",
    "    ham_unique = list(set(header_ham).difference(set(header_spam)))\n",
    "    header_ham1 = pd.DataFrame(ham_unique)\n",
    "    header_ham1.to_csv(\"ham_unique.csv\")\n",
    "    \n",
    "\n",
    "    # Find the intersection of the unique ham words and the top 100 features, \n",
    "    # It indicates important words that are unique to ham messages.\n",
    "    with open(\"x2result.csv\", \"r\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        top100_features = []\n",
    "        for row in reader:\n",
    "            top100_features.append(row[1])\n",
    "    top100_features = top100_features[1:]    \n",
    "    \n",
    "    ham_unique_in_top = list(set(ham_unique).intersection(set(top100_features)))\n",
    "    \n",
    "    words14str = \"\"\n",
    "    for item in ham_unique_in_top:\n",
    "        words14str = words14str + \" \" + item\n",
    "    return words14str, spam, ham, spam_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9e3VAwouiQK"
   },
   "source": [
    "## **7. Crafting Adversarial Emails & Attacking SVM**\n",
    "We can insert the identified \"magic words\" to original spam emails. This proccess is what we called \"crafting adversarial emails\". Then, we feed the new feature vectors of these crafted emails to the SVM classifier to see if they can be misclassified as ham emails.\n",
    "\n",
    "**Run the code block below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "PxY8tJakw2yv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "m2_empty = pd.DataFrame()\n",
    "spam_cnt = 0\n",
    "threads = []\n",
    "m2_empty_l1 = pd.DataFrame()\n",
    "m2_empty_l2 = pd.DataFrame()\n",
    "m2_empty_l3 = pd.DataFrame()\n",
    "m2_empty_l4 = pd.DataFrame()\n",
    "m2_list = [m2_empty_l1, m2_empty_l2, m2_empty_l3, m2_empty_l4]\n",
    "\n",
    "class myThread(threading.Thread):\n",
    "\n",
    "    def __init__(self, threadID, name, spam_message, words14str, method, feature_model, feature_names, scaler, clf_lin, list_index, selection_model):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.threadID = threadID\n",
    "        self.name = name\n",
    "        self.spam_message = spam_message\n",
    "        self.words14str = words14str\n",
    "        self.method = method\n",
    "        self.feature_model = feature_model\n",
    "        self.feature_names = feature_names\n",
    "        self.scaler = scaler\n",
    "        self.clf_lin = clf_lin\n",
    "        self.list_index = list_index\n",
    "        self.lock = threading.Lock()\n",
    "        self.selection_model = selection_model\n",
    "\n",
    "    def run(self):\n",
    "        global spam_cnt  # Count the number of successful attacks (spam messages that are misclassified as ham).\n",
    "        print(\"Starting \" + self.name)\n",
    "        spam_cnt_1 = m2_empty_out(self.name, self.spam_message, self.words14str, self.method,\n",
    "                                  self.feature_model, self.feature_names, self.scaler, self.clf_lin,\n",
    "                                  self.list_index, self.selection_model)\n",
    "        spam_cnt = spam_cnt+spam_cnt_1\n",
    "        print(\"spam_cnt:\", spam_cnt)\n",
    "        time.sleep(0.1)\n",
    "        print(\"Exiting \" + self.name)\n",
    "\n",
    "# Crafting Email\n",
    "def m2_empty_out(name, spam_message, words14str, method, feature_model, feature_names, scaler, clf_lin, list_index, selection_model):\n",
    "    m2_empty_1 = pd.DataFrame()  # Store messages that successfully deceived the model\n",
    "    spam_cnt_1 = 0               # The number of successful deceptions\n",
    "    global m2_list               # Store results from different threads\n",
    "    \n",
    "    for j in spam_message.message:\n",
    "        choose_email = [j + words14str]                                    # append magic words\n",
    "        message_14_email = pd.DataFrame(choose_email, columns=[\"message\"]) # Adversarial Email\n",
    "        message_14_tf_idf = single_transform(                              # Convet email to vector\n",
    "            message_14_email[\"message\"], method, feature_model, feature_names, scaler, selection_model)\n",
    "\n",
    "        message_14_tf_idf = pd.DataFrame(\n",
    "            message_14_tf_idf.toarray(), columns=feature_names)\n",
    "        message_14_y = [1]\n",
    "        message_14_y = pd.Series(message_14_y)\n",
    "        message_CData = CDataset(message_14_tf_idf, message_14_y)\n",
    "        message_14_pred = clf_lin.predict(message_CData.X)\n",
    "        \n",
    "        # successful deception\n",
    "        if message_14_pred == 0:\n",
    "            spam_cnt_1 = spam_cnt_1 + 1\n",
    "            m2_empty_1 = m2_empty_1.append(\n",
    "                message_14_tf_idf, ignore_index=True)\n",
    "\n",
    "    m2_list[list_index] = m2_list[list_index].append(\n",
    "        m2_empty_1, ignore_index=True)\n",
    "\n",
    "    return spam_cnt_1\n",
    "\n",
    "\n",
    "\n",
    "def svm_attack(method, clf_lin, spam, words14str, feature_model, feature_names, scaler, selection_model):\n",
    "\n",
    "    global m2_empty\n",
    "\n",
    "    spam_messages = np.array_split(spam, 4)\n",
    "    print(\"Start processing message\")\n",
    "    thread1 = myThread(1, \"Thread-1\", spam_messages[0], words14str,\n",
    "                       method, feature_model, feature_names, scaler, clf_lin, 0, selection_model)\n",
    "    thread2 = myThread(2, \"Thread-2\", spam_messages[1], words14str,\n",
    "                       method, feature_model, feature_names, scaler, clf_lin, 1, selection_model)\n",
    "    thread3 = myThread(3, \"Thread-3\", spam_messages[2], words14str,\n",
    "                       method, feature_model, feature_names, scaler, clf_lin, 2, selection_model)\n",
    "    thread4 = myThread(4, \"Thread-4\", spam_messages[3], words14str,\n",
    "                       method, feature_model, feature_names, scaler, clf_lin, 3, selection_model)\n",
    "    threads.append(thread1)\n",
    "    threads.append(thread2)\n",
    "    threads.append(thread3)\n",
    "    threads.append(thread4)\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    m2_empty = m2_empty.append(m2_list[0], ignore_index=True)\n",
    "    m2_empty = m2_empty.append(m2_list[1], ignore_index=True)\n",
    "    m2_empty = m2_empty.append(m2_list[2], ignore_index=True)\n",
    "    m2_empty = m2_empty.append(m2_list[3], ignore_index=True)\n",
    "\n",
    "    print(\"Exiting Main Thread\")\n",
    "    print('Number of samples provided:', len(spam))\n",
    "    print('Number of crafted sample that got misclassified:', spam_cnt)\n",
    "    print('Success rate(Attack): {:.2f}%'.format((spam_cnt / len(spam)) * 100))\n",
    "    return m2_empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KIQP6h8VzVq"
   },
   "source": [
    "## **Experience 1**\n",
    "### **Individual Datasets** ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KIQP6h8VzVq",
    "tags": []
   },
   "source": [
    "### **1. Train target Model（SVM）** ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_classifier(FE_method, classifier, X_train, X_val, X_test, Y_train, Y_val, Y_test):\n",
    "    print('\\n===================== Step 1: Preprocessing ========================')\n",
    "    x_train, x_val, x_test = preprocess(X_train, X_val, X_test)\n",
    "    \n",
    "    print('\\n===================== Step 2: Feature Extraction ===================')\n",
    "    x_train_features, x_test_features, x_val_features, feature_names, feature_model, scalar = feature_extraction(x_train, x_test, x_val, FE_method)\n",
    "    \n",
    "    print('\\n===================== Step 3: Training Classifier ==================')\n",
    "\n",
    "    if classifier == 'SVM':\n",
    "        tr_set, clf = train_SVM(x_train_features, x_test_features, Y_train, Y_test)\n",
    "    elif classifier == 'LR':\n",
    "        tr_set, clf = train_LR(x_train_features, x_test_features, Y_train, Y_test)\n",
    "    else:\n",
    "        print('Please specify SVM or LR or DT as your classifier.')\n",
    "        \n",
    "    v_set = CDataset(x_val_features, Y_val)\n",
    "    return tr_set, v_set, clf, x_train_features, feature_names, feature_model, scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================== Train target Model（SVM） =====================\n",
      "2023-11-16 23:29:23,263 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2396976182.py:2: DtypeWarning: Columns (0,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,294,295,296,297,298,299,300,301,302,303,304,305,306,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('./enron_spam.csv', encoding='utf-8', encoding_errors='ignore')\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'ham'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2396976182.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n===================== Train target Model（SVM） ====================='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./enron_spam.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# X_train_shadow, Y_train_shadow  = copy_dataset_by_similarity(X_train, Y_train, 0.4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Total number of data:          '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/3269144314.py\u001b[0m in \u001b[0;36mdata_extraction\u001b[0;34m(df, valid)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;31m# We first separate the entire dataset to 80% and 20%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   5910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5911\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5912\u001b[0;31m             \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5913\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     def convert(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_failures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_coerce_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m         \u001b[0;31m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;31m# in pandas we don't store numpy str dtypes, so convert to object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \u001b[0;31m# work around NumPy brokenness, #1987\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype_intsafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# if we have a datetime/timedelta array of objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.astype_intsafe\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'ham'"
     ]
    }
   ],
   "source": [
    "print('\\n===================== Train target Model（SVM） =====================')\n",
    "df = pd.read_csv('./enron_spam.csv', encoding='utf-8', encoding_errors='ignore')\n",
    "X_train, X_val, X_test, Y_train, Y_val, Y_test = data_extraction(df)\n",
    "# X_train_shadow, Y_train_shadow  = copy_dataset_by_similarity(X_train, Y_train, 0.4)\n",
    "print('Total number of data:          ', str(len(X_train) + len(X_test) + len(X_val)))\n",
    "print('Number of data for training:    ' + str(len(X_train)))\n",
    "print('Number of data for testing:     ' + str(len(X_test)))\n",
    "print('Number of data for validation: ', str(len(X_val)))\n",
    "# print('Number of data for X_train_shadow: ', str(len(X_train_shadow)))\n",
    "# print('Number of data for Y_train_shadow: ', str(len(Y_train_shadow)))\n",
    "\n",
    "\n",
    "# Target modal\n",
    "T_FE_method= 'tfidf'\n",
    "T_classifier= 'SVM'\n",
    "T_tr_set, T_v_set, T_clf, T_x_train_features, T_feature_names, T_feature_model, T_scalar = train_classifier(T_FE_method, T_classifier, X_train, X_val, X_test, Y_train, Y_val, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34D9Gts3bKTv"
   },
   "source": [
    "### **3. Train Shadow Model & Adversarial Attack** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "penr97JhaHgq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adversarial_attack(S_clf, S_x_train_features, S_tr_set, S_v_set, S_feature_names, X_train, X_val, X_test, Y_train, Y_val, Y_test): \n",
    "    print('\\n===================== Step 4: PGD Attack ===========================')\n",
    "    # Run PGD attacks on the trained classifier with 100 spam emails.\n",
    "    lb = np.ndarray.min(S_x_train_features.toarray())\n",
    "    ub = np.ndarray.max(S_x_train_features.toarray())\n",
    "    attack_amount = 100\n",
    "    PGD_dmax = 0.06\n",
    "    result, cnt, ad_success_x, ori_dataframe, ori_examples2_y, successful_rate = pgd_attack(S_clf, S_tr_set, S_v_set, Y_val, S_feature_names, attack_amount, PGD_dmax, lb, ub)\n",
    "\n",
    "    print('\\n===================== Step 5: Magic Words ===========================')\n",
    "    words14str, spam, ham, spam_test = magical_word(X_train, X_val, X_test, Y_train, Y_val, Y_test, result, cnt)\n",
    "    print(words14str)\n",
    "\n",
    "    return words14str, spam, ham, spam_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================== Train target Model（SVM） =====================\n",
      "Number of data for shadow:  2173\n",
      "\n",
      "===================== Step 1: Preprocessing ========================\n",
      "Remove hyperlink...\n",
      "Merge into one line...\n",
      "Convert to lowercase...\n",
      "Remove number...\n",
      "Remove punctuation...\n",
      "Remove whitespace...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenize...\n",
      "Remove stop words...\n",
      "Lemmatize...\n",
      "\n",
      "===================== Step 2: Feature Extraction ===================\n",
      "2023-11-16 21:24:51,850 - py.warnings - WARNING - /Users/yixuan/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "\n",
      "\n",
      "===================== Step 3: Training Classifier ==================\n",
      "Start training SVM:\n",
      "The best training parameters are:  [('C', 10)]\n",
      "Validation Accuracy: 98.67549668874173%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TN   FP]\n",
      "[FN   TP]\n",
      "True Negatives (TN): 262\n",
      "False Positives (FP): 0\n",
      "False Negatives (FN): 4\n",
      "True Positives (TP): 36\n",
      "False Negative Rate (FNR = FN / (FN + TP)): 10.00%\n",
      "\n",
      "===================== Step 4: PGD Attack ===========================\n",
      "Success rate of PGD(validation): 28.00%\n",
      "\n",
      "===================== Step 5: Magic Words ===========================\n",
      " resolut renew infobfontap divx ham facem stylebord strip footer blew buycom cccccc uranium iv belongingto sweepstak builtin wrote motley cnet plea valigntoptd fool geneva lia ryanair season monday\n",
      "Number of magic words: 28\n",
      "Set of magic words:\n",
      " ['resolut', 'renew', 'infobfontap', 'divx', 'ham', 'facem', 'stylebord', 'strip', 'footer', 'blew', 'buycom', 'cccccc', 'uranium', 'iv', 'belongingto', 'sweepstak', 'builtin', 'wrote', 'motley', 'cnet', 'plea', 'valigntoptd', 'fool', 'geneva', 'lia', 'ryanair', 'season', 'monday']\n",
      "\n",
      "============== Attacking Target Model ===============\n",
      "Start processing message\n",
      "Starting Thread-1\n",
      "Starting Thread-2\n",
      "Starting Thread-3\n",
      "Starting Thread-4\n",
      "2023-11-16 21:25:27,676 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:25:27,698 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:25:27,701 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:25:27,714 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:25:27,742 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:25:27,770 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:25:27,850 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:25:27,862 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:25:27,912 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:25:27,949 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:25:28,034 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:25:28,054 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:25:28,198 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:25:28,229 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:25:28,322 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:25:28,421 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:25:28,630 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:25:28,654 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:25:28,821 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:25:28,915 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:25:28,950 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:65: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_list[list_index] = m2_list[list_index].append(\n",
      "\n",
      "2023-11-16 21:25:28,972 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "spam_cnt: 7\n",
      "2023-11-16 21:25:29,084 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "Exiting Thread-4\n",
      "2023-11-16 21:25:29,105 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:25:29,117 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:65: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_list[list_index] = m2_list[list_index].append(\n",
      "\n",
      "spam_cnt: 15\n",
      "spam_cnt: 23\n",
      "spam_cnt: 33\n",
      "Exiting Thread-1\n",
      "Exiting Thread-3\n",
      "Exiting Thread-2\n",
      "2023-11-16 21:25:29,316 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:95: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty = m2_empty.append(m2_list[0], ignore_index=True)\n",
      "\n",
      "2023-11-16 21:25:29,327 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:96: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty = m2_empty.append(m2_list[1], ignore_index=True)\n",
      "\n",
      "2023-11-16 21:25:29,346 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:97: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty = m2_empty.append(m2_list[2], ignore_index=True)\n",
      "\n",
      "2023-11-16 21:25:29,368 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:98: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty = m2_empty.append(m2_list[3], ignore_index=True)\n",
      "\n",
      "Exiting Main Thread\n",
      "Number of samples provided: 40\n",
      "Number of crafted sample that got misclassified: 33\n",
      "Success rate(Attack): 82.50%\n"
     ]
    }
   ],
   "source": [
    "print('\\n===================== Train target Model（SVM） =====================')\n",
    "X_train_shadow, Y_train_shadow  = copy_dataset_by_similarity(X_train, Y_train, 1)\n",
    "print('Number of data for shadow: ', str(len(X_train_shadow)))\n",
    "\n",
    "S_FE_method= 'tfidf'\n",
    "S_classifier= 'SVM'\n",
    "S_tr_set, S_v_set, S_clf, S_x_train_features, S_feature_names, S_feature_model, S_scalar = train_classifier(S_FE_method, S_classifier, X_train_shadow, X_val, X_test, Y_train_shadow, Y_val, Y_test)\n",
    "\n",
    "words14str, spam, ham, spam_test = adversarial_attack(S_clf, S_x_train_features, S_tr_set, S_v_set, S_feature_names, X_train_shadow, X_val, X_test, Y_train_shadow, Y_val, Y_test)\n",
    "magic_words = words14str.lstrip().split(' ')\n",
    "print('Number of magic words:', len(magic_words))\n",
    "print('Set of magic words:\\n', magic_words)\n",
    "\n",
    "print('\\n============== Attacking Target Model ===============')\n",
    "# we craft a set of spam emails and feed them to the trained classifier for testing. \n",
    "m2_empty = svm_attack(T_FE_method, T_clf, spam_test, words14str, T_feature_model, T_feature_names, T_scalar, 'NaN')  # spam email from test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============== Attacking Shadow Model ===============\n",
      "Start processing message\n",
      "Starting Thread-1\n",
      "Starting Thread-2\n",
      "Starting Thread-3\n",
      "2023-11-16 21:26:00,038 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "Starting Thread-4\n",
      "2023-11-16 21:26:00,073 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:26:00,083 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:26:00,118 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-16 21:26:00,144 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:26:00,168 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:26:00,253 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:26:00,287 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:26:00,360 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:26:00,373 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:26:00,430 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:26:00,583 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:26:00,622 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:26:00,715 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:26:00,728 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:26:00,734 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:26:00,776 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:26:00,851 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:26:00,875 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:26:00,929 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:26:01,041 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:26:01,061 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:26:01,081 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:26:01,146 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:26:01,150 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:65: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_list[list_index] = m2_list[list_index].append(\n",
      "\n",
      "spam_cnt: 8\n",
      "2023-11-16 21:26:01,232 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "2023-11-16 21:26:01,267 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:65: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_list[list_index] = m2_list[list_index].append(\n",
      "\n",
      "spam_cnt:spam_cnt: 24\n",
      " 16\n",
      "2023-11-16 21:26:01,309 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty_1 = m2_empty_1.append(\n",
      "\n",
      "Exiting Thread-1\n",
      "2023-11-16 21:26:01,334 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:65: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_list[list_index] = m2_list[list_index].append(\n",
      "\n",
      "spam_cnt: 34\n",
      "Exiting Thread-3\n",
      "Exiting Thread-4\n",
      "Exiting Thread-2\n",
      "2023-11-16 21:26:01,452 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:95: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty = m2_empty.append(m2_list[0], ignore_index=True)\n",
      "\n",
      "2023-11-16 21:26:01,461 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:96: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty = m2_empty.append(m2_list[1], ignore_index=True)\n",
      "\n",
      "2023-11-16 21:26:01,480 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:97: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty = m2_empty.append(m2_list[2], ignore_index=True)\n",
      "\n",
      "2023-11-16 21:26:01,510 - py.warnings - WARNING - /var/folders/py/tjrmszvx6nl3krj0kb3jq1hw0000gn/T/ipykernel_77637/2321190388.py:98: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  m2_empty = m2_empty.append(m2_list[3], ignore_index=True)\n",
      "\n",
      "Exiting Main Thread\n",
      "Number of samples provided: 40\n",
      "Number of crafted sample that got misclassified: 34\n",
      "Success rate(Attack): 85.00%\n"
     ]
    }
   ],
   "source": [
    "print('\\n============== Attacking Shadow Model ===============')\n",
    "# we craft a set of spam emails and feed them to the trained classifier for testing. \n",
    "m2_empty = svm_attack(S_FE_method, S_clf, spam_test, words14str, S_feature_model, S_feature_names, S_scalar, 'NaN')  # spam email from test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KIQP6h8VzVq"
   },
   "source": [
    "## **Experience 2**\n",
    "### **Combine Datasets** ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n===================== Train target Model（SVM） =====================')\n",
    "df = pd.read_csv('./large_dataset.csv', encoding='utf-8', encoding_errors='ignore')\n",
    "X_train, X_val, X_test, Y_train, Y_val, Y_test = data_extraction(df)\n",
    "print('Total number of data:          ', str(len(X_train) + len(X_test) + len(X_val)))\n",
    "print('Number of data for training:    ' + str(len(X_train)))\n",
    "print('Number of data for testing:     ' + str(len(X_test)))\n",
    "print('Number of data for validation: ', str(len(X_val)))\n",
    "X_train_shadow, Y_train_shadow  = copy_dataset_by_similarity(X_train, Y_train, 0.2)\n",
    "\n",
    "\n",
    "# Target modal\n",
    "T_FE_method= 'tfidf'\n",
    "T_classifier= 'SVM'\n",
    "T_tr_set, T_v_set, T_clf, T_x_train_features, T_feature_names, T_feature_model, T_scalar = train_classifier(T_FE_method, T_classifier, X_train, X_val, X_test, Y_train, Y_val, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "penr97JhaHgq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adversarial_attack2(T_FE_method, S_FE_method, classifier, df_shadow, PGD_dmax, T_feature_model, T_feature_names, T_scalar):\n",
    "    print('\\n===================== Step 1: Data Extraction =====================')\n",
    "    X_train, X_val, X_test, Y_train, Y_val, Y_test = data_extraction(df_shadow)\n",
    "    print('Total number of data:          ', str(len(X_train) + len(X_test) + len(X_val)))\n",
    "    print('Number of data for training:    ' + str(len(X_train)))\n",
    "    print('Number of data for testing:     ' + str(len(X_test)))\n",
    "    print('Number of data for validation: ', str(len(X_val)))\n",
    "    \n",
    "    print('\\n===================== Step 2: Preprocessing ========================')\n",
    "    x_train, x_val, x_test = preprocess(X_train, X_val, X_test)\n",
    "    \n",
    "    print('\\n===================== Step 3: Feature Extraction ===================')\n",
    "    x_train_features, x_test_features, x_val_features, feature_names, feature_model, scalar = feature_extraction(x_train, x_test, x_val, S_FE_method)\n",
    "    \n",
    "    print('\\n===================== Step 4: Training Classifier ==================')\n",
    "    if classifier == 'SVM':\n",
    "        tr_set, v_set, clf = train_SVM(x_train_features, x_test_features, x_val_features, Y_train, Y_test, Y_val)\n",
    "    elif classifier == 'LR':\n",
    "        tr_set, v_set, clf = train_LR(x_train_features, x_test_features, x_val_features, Y_train, Y_test, Y_val)\n",
    "    else:\n",
    "        print('Please specify SVM or LR or DT as your classifier.')\n",
    "        \n",
    "    print('\\n===================== Step 5: PGD Attack ===========================')\n",
    "    # Run PGD attacks on the trained classifier with 100 spam emails.\n",
    "    lb = np.ndarray.min(x_train_features.toarray())\n",
    "    ub = np.ndarray.max(x_train_features.toarray())\n",
    "    attack_amount = 100\n",
    "    result, cnt, ad_success_x, ori_dataframe, ori_examples2_y, successful_rate = pgd_attack(clf, tr_set, v_set, Y_val, feature_names, attack_amount, PGD_dmax, lb, ub)\n",
    "    \n",
    "    print('\\n===================== Step 6: Magic Words ===========================')\n",
    "    words14str, spam, ham = magical_word(X_train, X_val, Y_train, Y_val, result, cnt)\n",
    "    print(words14str)\n",
    "\n",
    "    print('\\n============== Step 7: Crafting Adversarial Emails & Attacking SVM ===============')\n",
    "    # we craft a set of spam emails and feed them to the trained classifier for testing. \n",
    "    # It prints out the success rate of this attack.\n",
    "    m2_empty = svm_attack(T_FE_method, T_clf, spam, words14str, T_feature_model, T_feature_names, T_scalar, 'NaN')  # spam email from test dataset\n",
    "    \n",
    "    return words14str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shadow model\n",
    "S_FE_method= 'word2vec'\n",
    "S_classifier= 'SVM'\n",
    "\n",
    "# adversarial attack\n",
    "PGD_dmax = 0.06\n",
    "words14str = adversarial_attack(T_FE_method, S_FE_method, S_classifier, df_shadow, PGD_dmax, T_feature_model, T_feature_names, T_scalar)\n",
    "magic_words = words14str.lstrip().split(' ')\n",
    "print('Number of magic words:', len(magic_words))\n",
    "print('Set of magic words:\\n', magic_words)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
