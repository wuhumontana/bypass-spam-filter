{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBdiD49e5Q7L"
      },
      "source": [
        "# **Lab 4: Adversarial Attacks Against Machine Learning Based Spam Filters**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mafdGzfSbQny"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPJYb7EA5a24"
      },
      "source": [
        "Please Type the Names of the Team Members:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSubQvScyKdQ"
      },
      "source": [
        "## **Introduction**\n",
        "Machine learning-based spam detection models learn from a set of labeled training data and detect spam emails. One class of vulnerabilities can help an attack to slightly modify the spam emails, called adversarial examples, to manipulate a trained model, e.g., a support vector machine(SVM) classifier, to misclassify maliciously during detection. However, feature extraction methods can make it difficult to translate numerical changes in the feature space, to needed changes to an email consisting of words.\n",
        "\n",
        "This lab uses a new attack method to understand how adversarial examples purposely modify the TF-IDF (term frequency-inverse document frequency) feature vector representing an email. A set of \"magic words\", or \"malicious words\" are identified from the TF-IDF vectors that experience the most significant changes made by a Projected Gradient Descent (PGD) algorithm. Adding these magic words to a spam email increases the chance for desirable misclassifications.\n",
        "\n",
        "## **Publications**\n",
        "\n",
        "For more information on this method, you can refer to the following publications:\n",
        "\n",
        "(1) C. Wang, D. Zhang, S. Huang, X. Li, and L. Ding, “Crafting Adversarial Email Content against Machine Learning Based Spam Email Detection,” In Proceedings of the 2021 International Symposium on Advanced Security on Software and Systems (ASSS ’21) with AsiaCCS 2021, Virtual Event, Hong Kong, June 7, 2021. [Download](https://isi.jhu.edu/wp-content/uploads/2021/04/ASSS_Workshop_Paper.pdf)\n",
        "\n",
        "(2) Q. Cheng, A. Xu, X. Li, and L. Ding, “Adversarial Email Generation against Spam Detection Models through Feature Perturbation,” The 2022 IEEE International Conference on Assured Autonomy (ICAA’22), Virtual Event, March 22-23, 2022. [Download](https://isi.jhu.edu/wp-content/uploads/2022/04/Adversarial_Attacks_Against_Machine_Learning_Based_SpamFilters__IEEE.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv-y6Ac6FYWO"
      },
      "source": [
        "## **1. Loading Dataset**\n",
        "The dataset to be used is called Ling-Spam. The Ling-Spam dataset is a collection of 2,893 spam and ham email messages curated from the Linguist List. These messages focus on linguistic interests around job postings, research opportunities, and software discussion. You can download this dataset below coming with the lab assignment.\n",
        "\n",
        "### Acknowledgements\n",
        "The dataset and its information come from the original authors of \"A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists\". \\\\\n",
        "\n",
        "**Run the code block below:**\n",
        "\n",
        "choose the message.csv to upload. Wait until it shows 100% before you continue. The below code is for the \"Google Colab\" environment, for another environment (like Jupyter Notebook), you can choose the corresponding upload function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXkAvUlp4fRP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "8fa88ce4-8dc1-4018-98aa-179a3a2ac984"
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2e057457-1d38-4e71-a3ba-e693c5c12676\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2e057457-1d38-4e71-a3ba-e693c5c12676\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving messages.csv to messages.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1re2gZWrle_P"
      },
      "source": [
        "**Run the code block below:**\n",
        "\n",
        "This splits the loaded dataset into three subsets of training, validation, and testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZvxGJk-rkkc"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def data_extraction():\n",
        "\n",
        "  # Change the 'messages.csv' to the filename you uploaded.\n",
        "  df = pd.read_csv('messages.csv')\n",
        "  x = df.message\n",
        "  y = df.label\n",
        "  # We first separate the entire dataset to 80% and 20%.\n",
        "  # Let the 80% of entire dataset becoming the first dataset(which will be split to traning dataset and the validation dataset), and let the 20% of entire dataset becoming the testing dataset.\n",
        "  x_train_val, x_test, y_train_val, y_test = train_test_split(x, y, test_size=0.2, random_state=99, stratify=y)\n",
        "  # Let the 80% of the train_val dataset be the traning dataset, and the 20% of the train_val dataset be the validation dataset.\n",
        "  x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.2, random_state=99, stratify=y_train_val)\n",
        "\n",
        "\n",
        "  return x_train, x_val, x_test, y_train, y_val, y_test\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, X_test, Y_train, Y_val, Y_test = data_extraction()\n",
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2aXVdK7DLuy",
        "outputId": "b5bd7d17-87b4-4e08-9d5b-f4d11189066c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2345    crossing boundaries : interdisciplinary approa...\n",
            "2664    this is not spam ; you are receiving this mess...\n",
            "1049    computational aspects of cognitive science nsf...\n",
            "1560    dear subscribers : the linguist list has just ...\n",
            "2167    i would like to know of sources for bengali so...\n",
            "                              ...                        \n",
            "2598    = 20 the virtual girlfriend and virtual boyfri...\n",
            "1310    learn to put angels to work ! angels are anoth...\n",
            "1906    call for papers sixth workshop on very large c...\n",
            "2286    for some time , i have been puzzled by a claim...\n",
            "191                       how to get on elsnet ? thanks\\n\n",
            "Name: message, Length: 1851, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFUv7AmsFW8e"
      },
      "source": [
        "In the code block above, we have read the dataset into variables x\n",
        "and y. Variable x contains the email body in a list of words and variable y contains the class labels with 0 being ham and 1 being spam."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We divide the entire data set randomly into three different data sets which are training data, validation data, and testing data. After we split the dataset twice: 64% of the entire dataset becomes the traning dataset(Y_train), 16% becomes the validation dataset(X_val), and 20% becomes the testing dataset(X_test).\n"
      ],
      "metadata": {
        "id": "_w-WXw69wAT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 1**\n",
        "\n",
        "Why do we need the three data subsets described above? Please explain what is a training dataset, a validation dataset, and a testing dataset. For some additional insights, you can refer to the article at https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7. (However, note that the validation dataset is also used to identify the magical words in this lab, kind of different from its typical use.)"
      ],
      "metadata": {
        "id": "yCGLR9neo8zL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T9JcMkUm0FI9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KJ7iGauJih3"
      },
      "source": [
        "## **2. Preprocessing the Emails**\n",
        "For preparation for use, we remove all the HTML tags, numbers, punctuation marks, and English stop words to keep only useful information. We also convert all the words to lowercase and each paragraph into a single line instead of multiple lines. In the last step of preprocessing, we conduct stemming on all the words. \\\\\n",
        "\n",
        "\\\\\n",
        "**Run the code block below:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfS6VpTaH7Wu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c74c6938-2a51-43d9-c8e0-157cf60452e9"
      },
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "# Remove the hyperlink.\n",
        "def remove_hyperlink(word):\n",
        "\n",
        "    return re.sub(r\"http\\S+\", \" \", word)\n",
        "\n",
        "\n",
        "# Convert the letter to lowercase.\n",
        "def to_lower(word):\n",
        "    result = word.lower()\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# Remove the numbers.\n",
        "def remove_number(word):\n",
        "    result = re.sub(r'\\d+', ' ', word)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# Remove the puncturations.\n",
        "def remove_punctuation(word):\n",
        "    result = word.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# Remove the whitespace.\n",
        "def remove_whitespace(word):\n",
        "    result = word.strip()\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# Merge multiple lines into one line.\n",
        "def replace_newline(word):\n",
        "\n",
        "    return word.replace('\\n', ' ')\n",
        "\n",
        "\n",
        "def clean_up_pipeline(sentence):\n",
        "    cleaning_utils = [remove_hyperlink, replace_newline, to_lower, remove_number, remove_punctuation, remove_whitespace]\n",
        "    for o in cleaning_utils:\n",
        "        sentence = o(sentence)\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "# Remove the stopwords, for example: a, and, an, above, ..., etc.\n",
        "def remove_stop_words(words):\n",
        "    result = [i for i in words if i not in ENGLISH_STOP_WORDS]\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# Reduce a word to its root word.\n",
        "def word_stemmer(words):\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    return [stemmer.stem(o) for o in words]\n",
        "\n",
        "\n",
        "# Remove inflectional endings only and to return the base.\n",
        "def word_lemmatizer(words):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    return [lemmatizer.lemmatize(o) for o in words]\n",
        "\n",
        "\n",
        "# Clear out the unnecessary information.\n",
        "def clean_token_pipeline(words):\n",
        "    cleaning_utils = [remove_stop_words, word_lemmatizer]\n",
        "\n",
        "    for o in cleaning_utils:\n",
        "        words = o(words)\n",
        "\n",
        "    return words\n",
        "\n",
        "\n",
        "def preprocess(X_train, X_val, X_test):\n",
        "    x_train_clean = [clean_up_pipeline(o) for o in X_train]\n",
        "    x_test_clean = [clean_up_pipeline(o) for o in X_test]\n",
        "    x_val_clean = [clean_up_pipeline(o) for o in X_val]\n",
        "    x_train_tokenize = [word_tokenize(o) for o in x_train_clean]\n",
        "    x_test_tokenize = [word_tokenize(o) for o in x_test_clean]\n",
        "    x_val_tokenize = [word_tokenize(o) for o in x_val_clean]\n",
        "    x_train_clean_token = [clean_token_pipeline(o) for o in x_train_tokenize]\n",
        "    x_test_clean_token = [clean_token_pipeline(o) for o in x_test_tokenize]\n",
        "    x_val_clean_token = [clean_token_pipeline(o) for o in x_val_tokenize]\n",
        "    x_train = [\" \".join(o) for o in x_train_clean_token]\n",
        "    x_test = [\" \".join(o) for o in x_test_clean_token]\n",
        "    x_val = [\" \".join(o) for o in x_val_clean_token]\n",
        "\n",
        "    return x_train, x_val, x_test\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoPue9h3Ik--",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebdc1d88-c1b1-4a84-8c61-49a5207ef57f"
      },
      "source": [
        "x_train, x_val, x_test = preprocess(X_train, X_val, X_test)\n",
        "print(x_train[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "crossing boundary interdisciplinary approach latin america th june nd july paper international conference aim explore contemporary cultural debate taking place latin america draw various strand debate multidisciplinary forum paper consider various issue modernization hybridity transculturation apply various field study paper welcome following field cultural study literature particularly looking trend contemporary narrative including neoavantgarde popular fiction drama study cinema gender study popular culture comparative literature anthropology ethnography sociology linguistics economics politics law symposium proposed far include exile latin american experience indigenismo negrismo u s latin america paper longer minute abstract word english spanish portuguese sent preferably email conference organiser department language cultural study university limerick ireland st january conference organizer nuala finnegan kate quinn nancy serrano department language cultural study university limerick limerick ireland tel fax email nuala finnegan ul kate quinn ul nancy serrano ul update visit webpage http www ul neylonm conf html mr michele j neylon department language cultural study university limerick limerick ireland tel http www ul neylonm index html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N68YIZqIJmc8"
      },
      "source": [
        "## **3. Feature Extraction**\n",
        "In this step, we convert the words of an email into a numerical feature vector, representing information of that email used for classification. Among many such methods, this lab will use TF-IDF. \\\\\n",
        "\n",
        "\\\\\n",
        "**Run the code block below:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlpwcsgxJrMK"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "\n",
        "def convert_to_feature(raw_tokenize_data):\n",
        "    raw_sentences = [' '.join(o) for o in raw_tokenize_data]\n",
        "\n",
        "\n",
        "    return vectorizer.transform(raw_sentences)\n",
        "\n",
        "\n",
        "def TfidfConvert(x_train, x_test, x_val):\n",
        "    x_train = [o.split(\" \") for o in x_train]\n",
        "    x_test = [o.split(\" \") for o in x_test]\n",
        "    x_val = [o.split(\" \") for o in x_val]\n",
        "    x_train_raw_sentences = [' '.join(o) for o in x_train]\n",
        "    x_val_raw_sentences = [' '.join(o) for o in x_val]\n",
        "    raw_sentences = x_train_raw_sentences + x_val_raw_sentences\n",
        "    vectorizer.fit(raw_sentences)\n",
        "    x_train_features = convert_to_feature(x_train)\n",
        "    x_test_features = convert_to_feature(x_test)\n",
        "    x_val_features = convert_to_feature(x_val)\n",
        "\n",
        "\n",
        "    return x_train_features, x_test_features, x_val_features\n",
        "\n",
        "\n",
        "def feature_extraction(x_train, x_test, x_val):\n",
        "    x_train_features, x_test_features, x_val_features = TfidfConvert(x_train, x_test, x_val)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "\n",
        "    return x_train_features, x_test_features, x_val_features, feature_names, vectorizer, 'NaN'\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_features, x_test_features, x_val_features, feature_names, feature_model, scalar = feature_extraction(x_train, x_test, x_val)\n",
        "print(x_train_features[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKUZs88oPQHI",
        "outputId": "44b8e842-dbcb-4c03-8374-fe605f11d6fe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 46915)\t0.04995978886526812\n",
            "  (0, 46730)\t0.026339130425117322\n",
            "  (0, 46165)\t0.03947622319623475\n",
            "  (0, 46079)\t0.06738406036802615\n",
            "  (0, 45493)\t0.03977700013803277\n",
            "  (0, 44952)\t0.11181744754489616\n",
            "  (0, 44510)\t0.05387580457773005\n",
            "  (0, 44287)\t0.05652241413306133\n",
            "  (0, 43822)\t0.39223155312672875\n",
            "  (0, 43108)\t0.05387580457773005\n",
            "  (0, 42967)\t0.0908369729508178\n",
            "  (0, 42072)\t0.030422891994938406\n",
            "  (0, 41738)\t0.06769249607229276\n",
            "  (0, 41428)\t0.048286013617079605\n",
            "  (0, 41191)\t0.04694320656689324\n",
            "  (0, 40461)\t0.19504586281354305\n",
            "  (0, 40305)\t0.06889008975885552\n",
            "  (0, 39907)\t0.03693578685843772\n",
            "  (0, 39508)\t0.04349896974284344\n",
            "  (0, 39172)\t0.05649942743338347\n",
            "  (0, 38027)\t0.16100523653488252\n",
            "  (0, 37928)\t0.030967851468981452\n",
            "  (0, 34352)\t0.17252789630588594\n",
            "  (0, 33761)\t0.04616507275000695\n",
            "  (0, 33177)\t0.0501113548214093\n",
            "  :\t:\n",
            "  (0, 13380)\t0.06972591406926394\n",
            "  (0, 12783)\t0.0246562433322136\n",
            "  (0, 12483)\t0.041289338232556635\n",
            "  (0, 11958)\t0.07062867309774853\n",
            "  (0, 11523)\t0.05888267005040001\n",
            "  (0, 11514)\t0.07670772728645273\n",
            "  (0, 10125)\t0.08933026508123723\n",
            "  (0, 9695)\t0.1069160716428519\n",
            "  (0, 9244)\t0.04292046002028455\n",
            "  (0, 9241)\t0.22520700443678604\n",
            "  (0, 9076)\t0.06889008975885552\n",
            "  (0, 8369)\t0.10861929050505036\n",
            "  (0, 8234)\t0.04777903087730166\n",
            "  (0, 8081)\t0.08352754338572378\n",
            "  (0, 8077)\t0.07062867309774853\n",
            "  (0, 7754)\t0.04494458941831987\n",
            "  (0, 7015)\t0.06889008975885552\n",
            "  (0, 4975)\t0.0522902557019614\n",
            "  (0, 2178)\t0.032362762060562905\n",
            "  (0, 2145)\t0.04741321069356958\n",
            "  (0, 1910)\t0.05387580457773005\n",
            "  (0, 1450)\t0.03684140583158037\n",
            "  (0, 1447)\t0.12876138006085366\n",
            "  (0, 925)\t0.039241246636864915\n",
            "  (0, 179)\t0.030829123189868607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MND-TjQRXkpq"
      },
      "source": [
        "### **Question 2**\n",
        "Please research TF-IDF online and provide a concise explanation of how it is done in your own words in one paragraph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAbh0HSVrCQd"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0RZeCBFLSH9"
      },
      "source": [
        "## **4. Training SVM**\n",
        "In this step, we will train a Support Vector Machine (SVM) model as a spam filter. Then we use the validation dataset to evaluate how it performs.\n",
        "\n",
        "**Run the code block below:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAeBdHNfLM7k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "735b6b08-60b9-4398-812e-8d0881830bfb"
      },
      "source": [
        "!pip install secml\n",
        "from secml.data import CDataset\n",
        "from secml.data.splitter import CDataSplitterKFold\n",
        "from secml.ml.classifiers import CClassifierSVM\n",
        "from secml.ml.peval.metrics import CMetricAccuracy\n",
        "from secml.ml.peval.metrics import CMetricConfusionMatrix\n",
        "from secml.adv.attacks.evasion import CAttackEvasionPGD\n",
        "# from Feature_extraction import single_transform\n",
        "import csv\n",
        "from statistics import mean, stdev\n",
        "import threading\n",
        "import time\n",
        "\n",
        "\n",
        "# We use the training data and validation data set to train the SVM model.\n",
        "def train_SVM(x_train_features, x_val_features, y_train, y_val):\n",
        "    tr_set = CDataset(x_train_features, y_train)\n",
        "    # Train the SVM\n",
        "    print(\"Build SVM\")\n",
        "    xval_splitter = CDataSplitterKFold()\n",
        "    clf_lin = CClassifierSVM()\n",
        "    xval_lin_params = {'C': [1]}\n",
        "    print(\"Find the best params\")\n",
        "    best_lin_params = clf_lin.estimate_parameters(\n",
        "        dataset=tr_set,\n",
        "        parameters=xval_lin_params,\n",
        "        splitter=xval_splitter,\n",
        "        metric='accuracy',\n",
        "        perf_evaluator='xval'\n",
        "    )\n",
        "    print(\"Finish Train\")\n",
        "    print(\"The best training parameters are: \", [\n",
        "          (k, best_lin_params[k]) for k in sorted(best_lin_params)])\n",
        "    print(\"Train SVM\")\n",
        "    clf_lin.fit(tr_set.X, tr_set.Y)\n",
        "\n",
        "    # Test the Classifier\n",
        "    v_set = CDataset(x_val_features, y_val)\n",
        "    y_pred = clf_lin.predict(v_set.X)\n",
        "    metric = CMetricAccuracy()\n",
        "    acc = metric.performance_score(y_true=v_set.Y, y_pred=y_pred)\n",
        "    confusion_matrix = CMetricConfusionMatrix()\n",
        "    cm = confusion_matrix.performance_score(y_true=v_set.Y, y_pred=y_pred)\n",
        "    print(\"Confusion Matrix: \")\n",
        "    print(cm)\n",
        "\n",
        "\n",
        "    return tr_set, v_set, clf_lin\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting secml\n",
            "  Downloading secml-0.15.6-py3-none-any.whl (463 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.0/464.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from secml) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from secml) (1.11.3)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.10/dist-packages (from secml) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from secml) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from secml) (1.3.2)\n",
            "Requirement already satisfied: Pillow>=6.2.1 in /usr/local/lib/python3.10/dist-packages (from secml) (9.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from secml) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from secml) (2.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->secml) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->secml) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->secml) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->secml) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->secml) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->secml) (3.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->secml) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->secml) (3.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->secml) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->secml) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->secml) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->secml) (2023.7.22)\n",
            "Installing collected packages: secml\n",
            "Successfully installed secml-0.15.6\n",
            "2023-11-04 01:55:06,026 - secml.settings - INFO - New `SECML_HOME_DIR` created: /root/secml-data\n",
            "2023-11-04 01:55:06,026 - secml.settings - INFO - New `SECML_HOME_DIR` created: /root/secml-data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:secml.settings:New `SECML_HOME_DIR` created: /root/secml-data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-04 01:55:06,039 - secml.settings - INFO - Default configuration file copied to: /root/secml-data/secml.conf\n",
            "2023-11-04 01:55:06,039 - secml.settings - INFO - Default configuration file copied to: /root/secml-data/secml.conf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:secml.settings:Default configuration file copied to: /root/secml-data/secml.conf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-04 01:55:06,045 - secml.settings - INFO - New `SECML_DS_DIR` created: /root/secml-data/datasets\n",
            "2023-11-04 01:55:06,045 - secml.settings - INFO - New `SECML_DS_DIR` created: /root/secml-data/datasets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:secml.settings:New `SECML_DS_DIR` created: /root/secml-data/datasets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-04 01:55:06,054 - secml.settings - INFO - New `SECML_MODELS_DIR` created: /root/secml-data/models\n",
            "2023-11-04 01:55:06,054 - secml.settings - INFO - New `SECML_MODELS_DIR` created: /root/secml-data/models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:secml.settings:New `SECML_MODELS_DIR` created: /root/secml-data/models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-04 01:55:06,059 - secml.settings - INFO - New `SECML_EXP_DIR` created: /root/secml-data/experiments\n",
            "2023-11-04 01:55:06,059 - secml.settings - INFO - New `SECML_EXP_DIR` created: /root/secml-data/experiments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:secml.settings:New `SECML_EXP_DIR` created: /root/secml-data/experiments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-04 01:55:06,065 - secml.settings - INFO - New `SECML_LOGS_DIR` created: /root/secml-data/logs\n",
            "2023-11-04 01:55:06,065 - secml.settings - INFO - New `SECML_LOGS_DIR` created: /root/secml-data/logs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:secml.settings:New `SECML_LOGS_DIR` created: /root/secml-data/logs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-04 01:55:06,071 - secml.settings - INFO - New `SECML_PYTORCH_DIR` created: /root/secml-data/pytorch-data\n",
            "2023-11-04 01:55:06,071 - secml.settings - INFO - New `SECML_PYTORCH_DIR` created: /root/secml-data/pytorch-data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:secml.settings:New `SECML_PYTORCH_DIR` created: /root/secml-data/pytorch-data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tr_set, v_set, clf_lin = train_SVM(x_train_features, x_val_features, Y_train, Y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi5HMdEVQ3a4",
        "outputId": "e48e7f8a-0c93-478c-bfe0-149be61e9cb4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build SVM\n",
            "Find the best params\n",
            "Finish Train\n",
            "The best training parameters are:  [('C', 1)]\n",
            "Train SVM\n",
            "Confusion Matrix: \n",
            "CArray([[386   0]\n",
            " [  6  71]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL1ZOAYQN12a"
      },
      "source": [
        "### **Question 3**\n",
        "Based on the provided confusion matrix in the output above, calculate the accuracy, false-positive rate, false-negative rate, true-positive rate, and true-negative rate. Please detail the necessary steps and include the results in the following text block. Additionally, provide an explanation for each of these metrics in the context of spam email detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-UG3wUYOaLG"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rKjzZTCO5N2"
      },
      "source": [
        "## **5. PGD Attack**\n",
        "Adversarial perturbations are made to input features, i.e., the TF-IDF values corresponding to words. We employ the Projected Gradient Descent (PGD) method to modify the feature values for desirable adversarial examples in the feature domain. PGD algorithm iteratively finds the needed changes with a constraint parameter, *dmax*, which is the Euclidean distance to the original input indicating the allowed extent of perturbation, to achieve the maximum loss in classification. In our approach, we run PGD over a set of randomly selected spam emails form the validation sataset to generate adversarial examples in the feature space. Then we test these modified TF-IDF vectors to see whether they could successfully bypass the detection. \\\\\n",
        "\n",
        "\\\\\n",
        "**Run the code block below:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:**\n",
        "These are explanations for the terms used in the code:\n",
        "\n",
        "1. `clf_lin` - SVM Classifier:\n",
        "   - Represents a Support Vector Machine (SVM) classifier for email classification.\n",
        "\n",
        "2. `tr_set` - Training Set:\n",
        "   - A dataset used for training the SVM classifier, containing input features and labels.\n",
        "\n",
        "3. `v_set` - Validation Set:\n",
        "   - A dataset used to identify the \"best\" magic words through PGD.\n",
        "\n",
        "4. `Y_val` - Validation Set Labels:\n",
        "   - Contains labels for the validation set, aiding in performance evaluation.\n",
        "\n",
        "5. `feature_names` - Name of the Features:\n",
        "   - Likely holds the names or labels of the dataset's features.\n",
        "\n",
        "6. `nb_attack` - Number of Attacks:\n",
        "   - Determines how many adversarial examples should be generated (the number of spam emails to modify by PGD).\n",
        "\n",
        "7. `dmax` - Maximum Perturbation Distance:\n",
        "   - Represents the maximum allowed change in feature values during adversarial attacks, measured as Euclidean distance.\n",
        "\n",
        "8. `lb` - Lower Bound:\n",
        "   - Sets a lower boundary on feature values, constraining perturbations during attacks.\n",
        "\n",
        "9. `ub` - Upper Bound:\n",
        "   - Defines an upper boundary on feature values, limiting perturbations during attacks.\n"
      ],
      "metadata": {
        "id": "Ix7GUWE6yAxQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCvPfWA-P1OG"
      },
      "source": [
        "def pgd_attack(clf_lin, tr_set, v_set, y_val, feature_names, nb_attack, dmax, lb, ub):\n",
        "\n",
        "    class_to_attack = 1\n",
        "    cnt = 0  # the number of success adversaril examples\n",
        "    ori_examples2_x = []\n",
        "    ori_examples2_y = []\n",
        "\n",
        "    for i in range(nb_attack):\n",
        "        # take a point at random being the starting point of the attack\n",
        "        idx_candidates = np.where(y_val == class_to_attack)\n",
        "        # select nb_init_pts points randomly in candidates and make them move\n",
        "        rn = np.random.choice(idx_candidates[0].size, 1)\n",
        "        x0, y0 = v_set[idx_candidates[0][rn[0]], :].X, v_set[idx_candidates[0][rn[0]], :].Y\n",
        "\n",
        "        x0 = x0.astype(float)\n",
        "        y0 = y0.astype(int)\n",
        "        x2 = x0.tondarray()[0]\n",
        "        y2 = y0.tondarray()[0]\n",
        "\n",
        "        ori_examples2_x.append(x2)\n",
        "        ori_examples2_y.append(y2)\n",
        "\n",
        "\n",
        "    # Perform adversarial attacks\n",
        "\n",
        "    noise_type = 'l2'  # Type of perturbation 'l1' or 'l2'\n",
        "\n",
        "    y_target = 0\n",
        "\n",
        "    # dmax = 0.09  # Maximum perturbation\n",
        "\n",
        "    # Bounds of the attack space. Can be set to `None` for unbounded\n",
        "    solver_params = {\n",
        "        'eta': 0.01,\n",
        "        'max_iter': 1000,\n",
        "        'eps': 1e-4}\n",
        "\n",
        "    # set lower bound and upper bound respectively to 0 and 1 since all features are Boolean\n",
        "    pgd_attack = CAttackEvasionPGD(\n",
        "        classifier=clf_lin,\n",
        "        double_init_ds=tr_set,\n",
        "        distance=noise_type,\n",
        "        dmax=dmax,\n",
        "        lb=lb, ub=ub,\n",
        "        solver_params=solver_params,\n",
        "        y_target=y_target\n",
        "    )\n",
        "\n",
        "    ad_examples_x = []\n",
        "    ad_examples_y = []\n",
        "    ad_index = []\n",
        "    cnt = 0\n",
        "\n",
        "    for i in range(len(ori_examples2_x)):\n",
        "        x0 = ori_examples2_x[i]\n",
        "        y0 = ori_examples2_y[i]\n",
        "        y_pred_pgd, _, adv_ds_pgd, _ = pgd_attack.run(x0, y0)\n",
        "\n",
        "        if y_pred_pgd.item() == 0:\n",
        "            cnt = cnt + 1\n",
        "            ad_index.append(i)\n",
        "\n",
        "        ad_examples_x.append(adv_ds_pgd.X.tondarray()[0])\n",
        "        ad_examples_y.append(y_pred_pgd.item())\n",
        "\n",
        "        attack_pt = adv_ds_pgd.X.tondarray()[0]\n",
        "\n",
        "    print(\"PGD attack successful rate:\", cnt / nb_attack)\n",
        "\n",
        "    startTime2 = time.time()\n",
        "    ori_examples2_x = np.array(ori_examples2_x)\n",
        "    ori_examples2_y = np.array(ori_examples2_y)\n",
        "    ad_examples_x = np.array(ad_examples_x)\n",
        "    ad_examples_y = np.array(ad_examples_y)\n",
        "\n",
        "    ori_dataframe = pd.DataFrame(ori_examples2_x, columns=feature_names)\n",
        "    ad_dataframe = pd.DataFrame(ad_examples_x, columns=feature_names)\n",
        "\n",
        "    # extract the success and fail examples\n",
        "\n",
        "    ad_dataframe['ad_label'] = ad_examples_y\n",
        "    ad_success = ad_dataframe.loc[ad_dataframe.ad_label == 0]\n",
        "    ori_success = ori_dataframe.loc[ad_dataframe.ad_label == 0]\n",
        "    ad_fail = ad_dataframe.loc[ad_dataframe.ad_label == 1]\n",
        "    ori_fail = ori_dataframe.loc[ad_dataframe.ad_label == 1]\n",
        "\n",
        "    ad_success_x = ad_success.drop(columns=['ad_label'])\n",
        "    ad_fail_x = ad_fail.drop(columns=['ad_label'])\n",
        "\n",
        "    result = (ad_success_x - ori_success)\n",
        "    ori_dataframe.to_csv('ori_dataframe.csv')\n",
        "    ad_dataframe.to_csv('ad_dataframe.csv')\n",
        "    result.to_csv('result.csv')\n",
        "\n",
        "\n",
        "\n",
        "    return result, cnt, ad_success_x, ori_dataframe, ori_examples2_y, cnt/nb_attack"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3qmM8uxrzrg"
      },
      "source": [
        "**Then run the code block below:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RYKIvm0QMHD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67c92c8e-e400-4c03-f6f1-caea2e3317af"
      },
      "source": [
        "lb = np.ndarray.min(x_train_features.toarray())\n",
        "ub = np.ndarray.max(x_train_features.toarray())\n",
        "attack_amount = 100\n",
        "dmax = 0.06\n",
        "result, cnt, ad_success_x, ori_dataframe, ori_examples2_y, successful_rate = pgd_attack(clf_lin, tr_set, v_set, Y_val, feature_names, attack_amount, dmax, lb, ub)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD attack successful rate: 0.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 4**\n",
        "Please explain how the success rate is calculated based on the code above using your own words. Then compare this success rate to the original false negative rate of this classifier on the validation set. What do you find?"
      ],
      "metadata": {
        "id": "Vf2zyWXxnsxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xGPj3veloy7h"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xUx6FBQiCBr"
      },
      "source": [
        "## **6. Identifying Magical Words**\n",
        "Adversarial emails are crafted by adding “magic words” to the original spam emails. The “magic words” are identified by intersecting the unique ham words with the “top words”. Specifically,  the unique ham words only appear in ham emails but not in spam emails.  After the  PGD  attacks on the set of randomly selected spam emails, we examine statistically which features are modified to the largest extent in the effort to bypass detection, to find their corresponding “top words”. (The changes are measured by calculating the variance of TF-IDF differences ver these spam emails before and after the PGD perturbation.) In  our  experiments,  we  use  the  top 100  words,  which  is relatively efficient. This set is relatively small and effective. \\\\\n",
        "\n",
        "\\\\\n",
        "**Run the code block below:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc552xOTSQZn"
      },
      "source": [
        "def magical_word(x_train, x_val, y_train, y_val, x_test, y_test, result, cnt):\n",
        "\n",
        "    # Method 2\n",
        "    x2result1 = result\n",
        "    x2result1 = np.array(x2result1)\n",
        "    x2result = result\n",
        "    x2result = x2result.multiply(x2result1)\n",
        "\n",
        "    sum_number = x2result.sum() / cnt\n",
        "    sum_number = pd.DataFrame(sum_number, columns=['sum_number'])\n",
        "    sum_number = sum_number.sort_values(\n",
        "        by='sum_number', ascending=False, inplace=False)\n",
        "\n",
        "    sum_number_pd = pd.DataFrame(sum_number.index[:100])\n",
        "    sum_number_pd.to_csv(\"x2result.csv\")\n",
        "    d = {'message': x_train, 'label': y_train}\n",
        "    df = pd.DataFrame(data=d)\n",
        "    d1 = {'message': x_val, 'label': y_val}\n",
        "    df1 = pd.DataFrame(data=d1)\n",
        "    frames = [df, df1]\n",
        "    messages = pd.concat(frames)\n",
        "    messages.to_csv(\"messages.csv\")\n",
        "    spam = messages[messages.label == 1]\n",
        "    ham = messages[messages.label == 0]\n",
        "\n",
        "    d2 = {'message': x_test, 'label': y_test}\n",
        "    df2 = pd.DataFrame(data=d2)\n",
        "    frames2 = [df2]\n",
        "    messages_test = pd.concat(frames2)\n",
        "    messages_test.to_csv(\"messages_test.csv\")\n",
        "    spam_test = messages_test[messages_test.label == 1]\n",
        "\n",
        "\n",
        "    # Tf-idf for spam datasets\n",
        "    vect_spam = TfidfVectorizer()\n",
        "    vect_spam.fit_transform(spam['message'])\n",
        "    header_spam = vect_spam.get_feature_names_out()\n",
        "\n",
        "    # Tf-idf for ham datasets\n",
        "    vect_ham = TfidfVectorizer()\n",
        "    vect_ham.fit_transform(ham['message'])\n",
        "    header_ham = vect_ham.get_feature_names_out()\n",
        "\n",
        "    # find unique ham words\n",
        "    ham_unique = list(set(header_ham).difference(set(header_spam)))\n",
        "    header_ham1 = pd.DataFrame(ham_unique)\n",
        "    header_ham1.to_csv(\"ham_unique.csv\")\n",
        "\n",
        "    with open(\"x2result.csv\", \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        top100_features = []\n",
        "        for row in reader:\n",
        "            top100_features.append(row[1])\n",
        "    top100_features = top100_features[1:]\n",
        "    # in ham & top100\n",
        "\n",
        "    ham_unique_in_top = list(\n",
        "        set(ham_unique).intersection(set(top100_features)))\n",
        "    words14str = \"\"\n",
        "    for item in ham_unique_in_top:\n",
        "        words14str = words14str + \" \" + item\n",
        "\n",
        "\n",
        "\n",
        "    return words14str, spam, ham, spam_test\n",
        "\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjldI2rTs_N5"
      },
      "source": [
        "**Run the code block below to list the magic words:**\n",
        "\n",
        "Variable words14str contains the identified magic words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yC_GNgts8UO",
        "outputId": "f3449565-36f7-4333-c5b2-96e590ea64b4"
      },
      "source": [
        "words14str, spam, ham, spam_test = magical_word(x_train, x_val, Y_train, Y_val, x_test, Y_test, result, cnt)\n",
        "print(words14str)\n",
        "print(len(words14str.split()))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " translation phonetic proceeding ammondt academic arizona french colingacl theory pkzip linguist euralex ipa linguistic cascadilla ldc sentence native benjamin workshop grammar risked chorus\n",
            "23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 5**\n",
        "(1). Based on your understanding after reading the paper using the link below, explain what is a \"good word\" attack?\n",
        "(Reference: https://www.ceas.cc/papers-2005/125.pdf)\n"
      ],
      "metadata": {
        "id": "lgS8EWh6qwSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pl1YCa2XyuTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2). The use of the good words is similar to our \"magic words\" in this approach.\n",
        "What is the difference in finding \"magic words\" from finding \"good words\"?\n",
        "\n"
      ],
      "metadata": {
        "id": "ulr-4hJpysdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "owxAMmiYyu5B"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdpkxWJ-RiOz"
      },
      "source": [
        "### **Task 1**\n",
        "In the code block below, try to run pgd_attack with different dmax values.\n",
        "\n",
        "What will happen when you try a higher dmax? Show the relationship by plotting dmax values and the corresponding PGD attack successful rate in a graph. You can do this by changing the code below with different dmax values. Try at least 5 dmax and explain your findings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgJtZ1gXSNUF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "6cb8b37e-b080-4e8f-87a4-a402ebdcf8b7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "attack_amount = 100\n",
        "dmax = 0.06\n",
        "result, cnt, ad_success_x, ori_dataframe, ori_examples2_y, successful_rate = pgd_attack(clf_lin, tr_set, v_set, Y_val, feature_names, attack_amount, dmax, lb, ub)\n",
        "\n",
        "# this plot cotains a single dmax vs the successful rate,\n",
        "# you can either write a for loop to try 5 or more dmax, or just manually\n",
        "# modify dmax values and record the results.\n",
        "list_dmax = [dmax]\n",
        "list_successful_rate = [successful_rate]\n",
        "plt.plot(list_dmax, list_successful_rate, 'b-o')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD attack successful rate: 0.21\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x78e70925e740>]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr71cHstiQGv"
      },
      "source": [
        "### **Task 2**\n",
        "How many magical words did you get in attacks? List the magical words in the following text block. Find the relationship between the number of magical words and dmax of PGD attack. Plot a graph to show this relationship. Try at least 5 different dmax values. Explain your findings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SglR24OotQvS"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "MUYgM6b2tVQw",
        "outputId": "a407ba54-058a-4af3-dace-23832818d115"
      },
      "source": [
        "attack_amount = 100\n",
        "dmax = 0.06\n",
        "result, cnt, ad_success_x, ori_dataframe, ori_examples2_y, successful_rate = pgd_attack(clf_lin, tr_set, v_set, Y_val, feature_names, attack_amount, dmax, lb, ub)\n",
        "words14str, spam, ham, spam_test = magical_word(x_train, x_val, Y_train, Y_val, x_test, Y_test, result, cnt)\n",
        "\n",
        "# this plot cotains a single dmax vs the number of magical words,\n",
        "# you can either write a for loop do to 5 or more dmax, or just manually\n",
        "# record dmax values\n",
        "list_dmax = [dmax]\n",
        "list_len = [len(words14str.split())]\n",
        "#print (words14str)\n",
        "#print(list_len)\n",
        "plt.plot(list_dmax, list_len, 'b-o')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD attack successful rate: 0.26\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x78e708f48370>]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAi4klEQVR4nO3de3BU5eH/8c8ScpPsLgbYXL4JEu5V1I4MImIj3wpJwEoQvypV1HSsIAYYVHSk06qttqmUqVYroC0NMgyiOAYzKNgQIOWuXBSpEInGgpCNEM0uRIiYPL8/GPbXlQDZJck+ie/XzBnJOc+enOcxmvecveAwxhgBAABYrFOkLwAAAOB8CBYAAGA9ggUAAFiPYAEAANYjWAAAgPUIFgAAYD2CBQAAWI9gAQAA1usc6QtoCY2NjTp06JCcTqccDkekLwcAADSDMUZHjx5VamqqOnU69z2UDhEshw4dUnp6eqQvAwAAhOHAgQNKS0s755gOESxOp1PSqQm7XK4IXw0AAGgOv9+v9PT0wO/xc+kQwXL6aSCXy0WwAADQzjTn5Ry86BYAAFiPYAEAANYjWAAAgPUIFgAAYD2CBQAAWI9gAQAA1iNYAACA9QgWAABgPYIFAABYj2ABAADWI1gAAID1CBYAAGA9ggUAAFiPYAEAANYjWAAAgPUIFgAAYD2CBQAAWI9gAQAA1iNYAACA9QgWAABgPYIFAABYj2ABAADWI1gAAID1CBYAAGA9ggUAAFiPYAEAANYjWAAAgPUIFgAAYD2CBQAAWI9gAQAA1iNYAACA9UIKloKCAg0ZMkROp1Mej0fjxo1TeXl5k2ONMRo9erQcDoeWL19+zvMaY/T4448rJSVF8fHxGjlypPbt2xfKpQEAgA4spGApKytTfn6+tmzZopKSEp08eVJZWVmqq6s7Y+xzzz0nh8PRrPPOnj1bzz//vObPn6+tW7eqS5cuys7O1okTJ0K5PAAA0EE5jDEm3AcfPnxYHo9HZWVlyszMDOz/4IMP9LOf/Uzbtm1TSkqKioqKNG7cuCbPYYxRamqqHn74Yc2cOVOS5PP5lJSUpIULF2rChAnnvQ6/3y+32y2fzyeXyxXudAAAQBsK5ff3Bb2GxefzSZISExMD+7755hvdcccdevHFF5WcnHzec1RWVsrr9WrkyJGBfW63W0OHDtXmzZubfEx9fb38fn/QBgAAOq6wg6WxsVEzZszQ8OHDNWjQoMD+Bx98UNdee61yc3ObdR6v1ytJSkpKCtqflJQUOPZ9BQUFcrvdgS09PT3MWQAAgPagc7gPzM/P1+7du7Vhw4bAvuLiYq1Zs0Y7d+5skYs7m1mzZumhhx4KfO33+4kWAAA6sLDusEydOlUrVqzQ2rVrlZaWFti/Zs0affrpp+ratas6d+6szp1P9dAtt9yiESNGNHmu008bVVdXB+2vrq4+61NKsbGxcrlcQRsAAOi4QgoWY4ymTp2qoqIirVmzRhkZGUHHH3vsMe3atUsffPBBYJOkZ599VoWFhU2eMyMjQ8nJySotLQ3s8/v92rp1q4YNGxbidAAAQEcU0lNC+fn5WrJkid566y05nc7Aa0zcbrfi4+OVnJzc5F2Rnj17BsXNwIEDVVBQoJtvvlkOh0MzZszQ008/rX79+ikjI0O/+c1vlJqaetZ3FgEAgB+WkIJl3rx5knTG0zuFhYXKy8tr9nnKy8sD7zCSpEcffVR1dXWaNGmSamtrdd1112nVqlWKi4sL5fIAAEAHdUGfw2ILPocFAID2p80+hwUAAKAtECwAAMB6BAsAALAewQIAAKxHsAAAAOsRLAAAwHoECwAAsB7BAgAArEewAAAA6xEsAADAegQLAACwHsECAACsR7AAAADrESwAAMB6BAsAALAewQIAAKxHsAAAAOsRLAAAwHoECwAAsB7BAgAArEewAAAA6xEsAADAegQLAACwHsECAACsR7AAAADrESwAAMB6BAsAALAewQIAAKxHsAAAAOsRLAAAwHoECwAAsB7BAgAArEewAAAA6xEsAADAegQLAACwHsECAACsR7AAAADrESwAAMB6BAsAALAewQIAAKxHsAAAAOsRLAAAwHoECwAAsB7BAgAArEewAAAA6xEsAADAegQLAACwHsECAACsR7AAAADrESwAAMB6BAsAALAewQIAAKxHsAAAAOsRLAAAwHoECwAAsB7BAgAArEewAAAA6xEsAADAegQLAACwHsECAACsR7AAAADrESwAAMB6IQVLQUGBhgwZIqfTKY/Ho3Hjxqm8vDxozOTJk9WnTx/Fx8erR48eys3N1d69e8953ry8PDkcjqAtJycn9NkAAIAOKaRgKSsrU35+vrZs2aKSkhKdPHlSWVlZqqurC4wZPHiwCgsLtWfPHr377rsyxigrK0sNDQ3nPHdOTo6qqqoC26uvvhrejAAAQIfjMMaYcB98+PBheTwelZWVKTMzs8kxu3bt0pVXXqmKigr16dOnyTF5eXmqra3V8uXLw7oOv98vt9stn88nl8sV1jkAAEDbCuX39wW9hsXn80mSEhMTmzxeV1enwsJCZWRkKD09/ZznWrdunTwejwYMGKApU6aopqbmrGPr6+vl9/uDNgAA0HGFHSyNjY2aMWOGhg8frkGDBgUdmzt3rhISEpSQkKCVK1eqpKREMTExZz1XTk6OFi1apNLSUj3zzDMqKyvT6NGjz/o0UkFBgdxud2A7XwwBAID2LeynhKZMmaKVK1dqw4YNSktLCzrm8/n05ZdfqqqqSnPmzNHBgwe1ceNGxcXFNevcn332mfr06aPVq1frhhtuOON4fX296uvrA1/7/X6lp6fzlBAAAO1Iqz8lNHXqVK1YsUJr1649I1Ykye12q1+/fsrMzNQbb7yhvXv3qqioqNnn7927t7p3766Kioomj8fGxsrlcgVtAACg4+ocymBjjKZNm6aioiKtW7dOGRkZzXqMMSbojsj5fPHFF6qpqVFKSkoolwcAADqokO6w5Ofna/HixVqyZImcTqe8Xq+8Xq+OHz8u6dRTOQUFBdq+fbv279+vTZs26dZbb1V8fLzGjBkTOM/AgQMDd1yOHTumRx55RFu2bNHnn3+u0tJS5ebmqm/fvsrOzm7BqQIAgPYqpGCZN2+efD6fRowYoZSUlMD22muvSZLi4uK0fv16jRkzRn379tXtt98up9OpTZs2yePxBM5TXl4eeIdRVFSUdu3apbFjx6p///669957NXjwYK1fv16xsbEtOFUAANBeXdDnsNiCz2EBAKD9abPPYQEAAGgLBAsAALAewQIAAKxHsAAAAOsRLAAAwHoECwAAsB7BAgAArEewAAAA6xEsAADAegQLAACwHsECAACsR7AAAADrESwAAMB6BAsAALAewQIAAKxHsAAAAOsRLAAAwHoECwAAsB7BAgAArEewAAAA6xEsAADAegQLAACwHsECAACsR7AAAADrESwAAMB6BAsAALAewQIAAKxHsAAAAOsRLAAAwHoECwAAsB7BAgAArEewAAAA6xEsAADAegQLAACwHsECAACsR7AAAADrESwAAMB6BAsAALAewQIAAKxHsAAAAOsRLAAAwHoECwAAsB7BAgAArEewAAAA6xEsAADAegQLAACwHsECAACsR7AAAADrESwAAMB6BAsAALAewQIAAKxHsAAAAOsRLAAAwHoECwAAsB7BAgAArEewAAAA6xEsAADAegQLAACwHsECAACsR7AAAADrESwAAMB6nSN9AQBwNg0N0vr1UlWVlJIi/eQnUlRUpK8KQCSEdIeloKBAQ4YMkdPplMfj0bhx41ReXh40ZvLkyerTp4/i4+PVo0cP5ebmau/evec8rzFGjz/+uFJSUhQfH6+RI0dq3759oc8GQIfx5ptSr17S//6vdMcdp/7Zq9ep/QB+eEIKlrKyMuXn52vLli0qKSnRyZMnlZWVpbq6usCYwYMHq7CwUHv27NG7774rY4yysrLU0NBw1vPOnj1bzz//vObPn6+tW7eqS5cuys7O1okTJ8KfGYB26803pf/7P+mLL4L3Hzx4aj/RAvzwOIwxJtwHHz58WB6PR2VlZcrMzGxyzK5du3TllVeqoqJCffr0OeO4MUapqal6+OGHNXPmTEmSz+dTUlKSFi5cqAkTJpz3Ovx+v9xut3w+n1wuV7jTAWCBhoZTd1K+HyunORxSWppUWcnTQ0B7F8rv7wt60a3P55MkJSYmNnm8rq5OhYWFysjIUHp6epNjKisr5fV6NXLkyMA+t9utoUOHavPmzU0+pr6+Xn6/P2gD0DGsX3/2WJEkY6QDB06NA/DDEXawNDY2asaMGRo+fLgGDRoUdGzu3LlKSEhQQkKCVq5cqZKSEsXExDR5Hq/XK0lKSkoK2p+UlBQ49n0FBQVyu92B7WwxBKD9qapq2XEAOoawgyU/P1+7d+/W0qVLzzh25513aufOnSorK1P//v112223tejrUWbNmiWfzxfYDhw40GLnBhBZKSktOw5AxxBWsEydOlUrVqzQ2rVrlZaWdsZxt9utfv36KTMzU2+88Yb27t2roqKiJs+VnJwsSaqurg7aX11dHTj2fbGxsXK5XEEbgI7hJz859RoVh6Pp4w6HlJ5+ahyAH46QgsUYo6lTp6qoqEhr1qxRRkZGsx5jjFF9fX2TxzMyMpScnKzS0tLAPr/fr61bt2rYsGGhXB6ADiAqSvrLX079+fvRcvrr557jBbfAD01IwZKfn6/FixdryZIlcjqd8nq98nq9On78uCTps88+U0FBgbZv3679+/dr06ZNuvXWWxUfH68xY8YEzjNw4MDAHReHw6EZM2bo6aefVnFxsT766CPdfffdSk1N1bhx41pupgDajfHjpTfekP7nf4L3p6Wd2j9+fGSuC0DkhPRJt/PmzZMkjRgxImh/YWGh8vLyFBcXp/Xr1+u5557T119/raSkJGVmZmrTpk3yeDyB8eXl5YF3GEnSo48+qrq6Ok2aNEm1tbW67rrrtGrVKsXFxV3A1AC0Z+PHS7m5fNItgFMu6HNYbMHnsAAA0P602eewAAAAtAWCBQAAWI9gAQAA1iNYAACA9QgWAABgPYIFAABYj2ABAADWI1gAAID1CBYAAGA9ggUAAFiPYAEAANYjWAAAgPUIFgAAYD2CBQAAWI9gAQAA1iNYAACA9QgWAABgPYIFAABYj2ABAADWI1gAAID1CBYAAGA9ggUAAFiPYAEAANYjWAAAgPUIFgAAYD2CBQAAWI9gAQAA1iNYAACA9QgWAABgPYIFAABYj2ABAADWI1gAAID1CBYAAGA9ggUAAFiPYAEAANYjWAAAgPUIFgAAYD2CBQAAWI9gAQAA1iNYAACA9QgWAABgPYIFAABYj2ABAADWI1gAAID1CBYAAGA9ggUAAFiPYAEAANYjWAAAgPUIFgAAYD2CBQAAWI9gAQAA1iNYAACA9QgWAABgPYIFAABYj2ABAADWI1gAAID1CBYAAGA9ggUAAFiPYAEAANYjWAAAgPUIFgAAYD2CBQAAWC+kYCkoKNCQIUPkdDrl8Xg0btw4lZeXB45/9dVXmjZtmgYMGKD4+Hj17NlT06dPl8/nO+d58/Ly5HA4gracnJzwZgQAADqckIKlrKxM+fn52rJli0pKSnTy5EllZWWprq5OknTo0CEdOnRIc+bM0e7du7Vw4UKtWrVK995773nPnZOTo6qqqsD26quvhjcjAADQ4TiMMSbcBx8+fFgej0dlZWXKzMxscsyyZcs0ceJE1dXVqXPnzk2OycvLU21trZYvXx7Wdfj9frndbvl8PrlcrrDOAQAA2lYov78v6DUsp5/qSUxMPOcYl8t11lg5bd26dfJ4PBowYICmTJmimpqaC7k0AADQgYR9h6WxsVFjx45VbW2tNmzY0OSYI0eOaPDgwZo4caJ+//vfn/VcS5cu1UUXXaSMjAx9+umn+tWvfqWEhARt3rxZUVFRZ4yvr69XfX194Gu/36/09HTusAAA0I6Ecocl7GCZMmWKVq5cqQ0bNigtLa3Jixg1apQSExNVXFys6OjoZp/7s88+U58+fbR69WrdcMMNZxx/8skn9dvf/vaM/QQLAADtR6s/JTR16lStWLFCa9eubTJWjh49qpycHDmdThUVFYUUK5LUu3dvde/eXRUVFU0enzVrlnw+X2A7cOBAONMAAADtxLlfWPI9xhhNmzZNRUVFWrdunTIyMs4Y4/f7lZ2drdjYWBUXFysuLi7ki/riiy9UU1OjlJSUJo/HxsYqNjY25PMCAID2KaQ7LPn5+Vq8eLGWLFkip9Mpr9crr9er48ePSzoVK6ff5rxgwQL5/f7AmIaGhsB5Bg4cqKKiIknSsWPH9Mgjj2jLli36/PPPVVpaqtzcXPXt21fZ2dktOFUAANBehXSHZd68eZKkESNGBO0vLCxUXl6eduzYoa1bt0qS+vbtGzSmsrJSvXr1kiSVl5cH3mEUFRWlXbt26ZVXXlFtba1SU1OVlZWlp556irsoAABA0gV+Dost+BwWAADanzb7HBYAAIC2QLAAAADrESwAAMB6BAsAALAewQIAAKxHsAAAAOsRLAAAwHoECwAAsB7BAgAArEewAAAA6xEsAADAegQLAACwHsECAACsR7AAAADrESwAAMB6BAsAALAewQIAAKxHsAAAAOsRLAAAwHoECwAAsB7BAgAArEewAAAA6xEsAADAegQLAACwHsECAACsR7AAAADrESwAAMB6BAsAALAewQIAAKxHsAAAAOsRLAAAwHoECwAAsB7BAgAArEewAAAA6xEsAADAegQLAACwHsECAACsR7AAAADrESwAAMB6BAsAALAewQIAAKxHsAAAAOsRLAAAwHoECwAAsB7BAgAArEewAAAA6xEsAADAegQLAACwHsECAACsR7AAAADrESwAAMB6BAsAALAewQIAAKxHsAAAAOsRLAAAwHoECwAAsB7BAgAArEewAAAA6xEsAADAegQLAACwHsECAACsR7AAAADrhRQsBQUFGjJkiJxOpzwej8aNG6fy8vLA8a+++krTpk3TgAEDFB8fr549e2r69Ony+XznPK8xRo8//rhSUlIUHx+vkSNHat++feHNCAAAdDghBUtZWZny8/O1ZcsWlZSU6OTJk8rKylJdXZ0k6dChQzp06JDmzJmj3bt3a+HChVq1apXuvffec5539uzZev755zV//nxt3bpVXbp0UXZ2tk6cOBH+zAAAQIfhMMaYcB98+PBheTwelZWVKTMzs8kxy5Yt08SJE1VXV6fOnTufcdwYo9TUVD388MOaOXOmJMnn8ykpKUkLFy7UhAkTznsdfr9fbrdbPp9PLpcr3OkAAIA2FMrv7wt6Dcvpp3oSExPPOcblcjUZK5JUWVkpr9erkSNHBva53W4NHTpUmzdvvpDLAwAAHUTTFdEMjY2NmjFjhoYPH65BgwY1OebIkSN66qmnNGnSpLOex+v1SpKSkpKC9iclJQWOfV99fb3q6+sDX/v9/lAvHwAAtCNh32HJz8/X7t27tXTp0iaP+/1+3Xjjjbr00kv15JNPhvttmlRQUCC32x3Y0tPTW/T8AADALmEFy9SpU7VixQqtXbtWaWlpZxw/evSocnJy5HQ6VVRUpOjo6LOeKzk5WZJUXV0dtL+6ujpw7PtmzZoln88X2A4cOBDONAAAQDsRUrAYYzR16lQVFRVpzZo1ysjIOGOM3+9XVlaWYmJiVFxcrLi4uHOeMyMjQ8nJySotLQ06x9atWzVs2LAmHxMbGyuXyxW0AQCAjiukYMnPz9fixYu1ZMkSOZ1Oeb1eeb1eHT9+XNL/j5W6ujotWLBAfr8/MKahoSFwnoEDB6qoqEiS5HA4NGPGDD399NMqLi7WRx99pLvvvlupqakaN25cy80UAAC0WyG96HbevHmSpBEjRgTtLywsVF5ennbs2KGtW7dKkvr27Rs0prKyUr169ZIklZeXB32Y3KOPPqq6ujpNmjRJtbW1uu6667Rq1arz3p0BAAA/DBf0OSy24HNYAABof9rsc1gAAADaAsECAACsR7AAAADrESwAAMB6BAsAALAewQIAAKxHsAAAAOsRLAAAwHoECwAAsB7BAgAArEewAAAA6xEsAADAegQLAACwHsECAACsR7AAAADrESwAAMB6BAsAALAewQIAAKxHsAAAAOsRLAAAwHoECwAAsB7BAgAArEewAAAA6xEsAADAegQLAACwHsECAACsR7AAAADrESwAAMB6BAsAALAewQIAAKxHsAAAAOt1jvQFtARjjCTJ7/dH+EoAAEBznf69ffr3+Ll0iGA5evSoJCk9PT3CVwIAAEJ19OhRud3uc45xmOZkjeUaGxt16NAhOZ1OORyOFj233+9Xenq6Dhw4IJfL1aLn7mhYq+ZjrZqPtWo+1io0rFfztdZaGWN09OhRpaamqlOnc79KpUPcYenUqZPS0tJa9Xu4XC5+oJuJtWo+1qr5WKvmY61Cw3o1X2us1fnurJzGi24BAID1CBYAAGA9guU8YmNj9cQTTyg2NjbSl2I91qr5WKvmY62aj7UKDevVfDasVYd40S0AAOjYuMMCAACsR7AAAADrESwAAMB6BAsAALBehw+WF198Ub169VJcXJyGDh2q995775zjly1bpoEDByouLk6XX3653nnnnaDjeXl5cjgcQVtOTk7g+Lp16844fnp7//33W2WOLaWt10qSPvnkE+Xm5qp79+5yuVy67rrrtHbt2hafW0uLxFrt2LFDo0aNUteuXdWtWzdNmjRJx44da/G5tbSWXitJ2rNnj8aOHSu3260uXbpoyJAh2r9/f+D4iRMnlJ+fr27duikhIUG33HKLqqurW3xurSES6/Xyyy9rxIgRcrlccjgcqq2tbelptYq2XquvvvpK06ZN04ABAxQfH6+ePXtq+vTp8vl8rTK/lhSJn6vJkyerT58+io+PV48ePZSbm6u9e/eGPwnTgS1dutTExMSYf/zjH+bf//63ue+++0zXrl1NdXV1k+M3btxooqKizOzZs83HH39sfv3rX5vo6Gjz0UcfBcbcc889Jicnx1RVVQW2r776KnC8vr4+6FhVVZX55S9/aTIyMkxjY2OrzzlckVgrY4zp16+fGTNmjPnwww/NJ598Yh544AFz0UUXmaqqqlad74WIxFodPHjQXHzxxeb+++83e/fuNe+995659tprzS233NLq870QrbFWFRUVJjEx0TzyyCNmx44dpqKiwrz11ltB57z//vtNenq6KS0tNdu2bTPXXHONufbaa1t9vhcqUuv17LPPmoKCAlNQUGAkma+//rq1p3rBIrFWH330kRk/frwpLi42FRUVprS01PTr14//Ds/yc/XSSy+ZsrIyU1lZabZv325uuukmk56ebr777ruw5tGhg+Xqq682+fn5ga8bGhpMamqqKSgoaHL8bbfdZm688cagfUOHDjWTJ08OfH3PPfeY3NzcZl/Dt99+a3r06GF+97vfhXbxbSwSa3X48GEjyfzrX/8K7PP7/UaSKSkpCXMmrS8Sa/XSSy8Zj8djGhoaAvt27dplJJl9+/aFOZPW1xprdfvtt5uJEyee9XvW1taa6Ohos2zZssC+PXv2GElm8+bN4U6lTURivf7b2rVr202wRHqtTnv99ddNTEyMOXnyZEiPa0u2rNWHH35oJJmKioqQHndah31K6Ntvv9X27ds1cuTIwL5OnTpp5MiR2rx5c5OP2bx5c9B4ScrOzj5j/Lp16+TxeDRgwABNmTJFNTU1Z72O4uJi1dTU6Be/+MUFzKZ1RWqtunXrpgEDBmjRokWqq6vTd999p5deekkej0eDBw9uwRm2nEitVX19vWJiYoL+crD4+HhJ0oYNGy54Xq2hNdaqsbFRb7/9tvr376/s7Gx5PB4NHTpUy5cvD4zfvn27Tp48GXSegQMHqmfPnmf9vjaI1Hq1Rzatlc/nk8vlUufOdv7VfLasVV1dnQoLC5WRkaH09PSw5tJhg+XIkSNqaGhQUlJS0P6kpCR5vd4mH+P1es87PicnR4sWLVJpaameeeYZlZWVafTo0WpoaGjynAsWLFB2dnar/+WMFyJSa+VwOLR69Wrt3LlTTqdTcXFx+vOf/6xVq1bp4osvbuFZtoxIrdVPf/pTeb1e/elPf9K3336rr7/+Wo899pgkqaqqqiWn2GJaY62+/PJLHTt2TH/84x+Vk5Ojf/7zn7r55ps1fvx4lZWVBc4RExOjrl27Nvv72iBS69Ue2bJWR44c0VNPPaVJkya1wKxaR6TXau7cuUpISFBCQoJWrlypkpISxcTEhDUXO5PQYhMmTAj8+fLLL9cVV1yhPn36aN26dbrhhhuCxn7xxRd699139frrr7f1ZVrhfGtljFF+fr48Ho/Wr1+v+Ph4/f3vf9dNN92k999/XykpKRG8+rZ1vrW67LLL9Morr+ihhx7SrFmzFBUVpenTpyspKem8fyV7R9LY2ChJys3N1YMPPihJ+vGPf6xNmzZp/vz5uv766yN5edZhvZov1LXy+/268cYbdemll+rJJ59s68uNqFDW6s4779SoUaNUVVWlOXPm6LbbbtPGjRsVFxcX8vftsP+n6969u6Kios54Z0B1dbWSk5ObfExycnJI4yWpd+/e6t69uyoqKs44VlhYqG7dumns2LFhzKDtRGqt1qxZoxUrVmjp0qUaPny4rrrqKs2dO1fx8fF65ZVXLnBWrSOSP1d33HGHvF6vDh48qJqaGj355JM6fPiwevfufQEzaj2tsVbdu3dX586ddemllwaN+dGPfhR4d0JycrK+/fbbM97pcr41j7RIrVd7FOm1Onr0qHJycuR0OlVUVKTo6OgLnVKrifRaud1u9evXT5mZmXrjjTe0d+9eFRUVhTWXDhssMTExGjx4sEpLSwP7GhsbVVpaqmHDhjX5mGHDhgWNl6SSkpKzjpdO3UWpqak5426AMUaFhYW6++67rf5hliK3Vt98840knXGHoFOnToGCt02kf66kU7dmExIS9NprrykuLk6jRo0KczatqzXWKiYmRkOGDFF5eXnQmE8++USXXHKJJGnw4MGKjo4OOk95ebn2799/zjWPtEitV3sUybXy+/3KyspSTEyMiouLw7pT0JZs+rkyp97oo/r6+vAmE9ZLdduJpUuXmtjYWLNw4ULz8ccfm0mTJpmuXbsar9drjDHmrrvuMo899lhg/MaNG03nzp3NnDlzzJ49e8wTTzwR9Fauo0ePmpkzZ5rNmzebyspKs3r1anPVVVeZfv36mRMnTgR979WrVxtJZs+ePW034QsQibU6fPiw6datmxk/frz54IMPTHl5uZk5c6aJjo42H3zwQdsvQjNF6ufqhRdeMNu3bzfl5eXmr3/9q4mPjzd/+ctf2nbyIWrptTLGmDfffNNER0ebl19+2ezbt8+88MILJioqyqxfvz4w5v777zc9e/Y0a9asMdu2bTPDhg0zw4YNa7uJhylS61VVVWV27txp/va3vwXeubdz505TU1PTdpMPUSTWyufzmaFDh5rLL7/cVFRUBH0MQbhv1W0LkVirTz/91PzhD38w27ZtM//5z3/Mxo0bzU033WQSExPP+nbq8+nQwWLMqf/J9+zZ08TExJirr77abNmyJXDs+uuvN/fcc0/Q+Ndff93079/fxMTEmMsuu8y8/fbbgWPffPONycrKMj169DDR0dHmkksuMffdd1/gX/p/+/nPf94uPvfhv0Vird5//32TlZVlEhMTjdPpNNdcc4155513WnWeLSESa3XXXXeZxMREExMTY6644gqzaNGiVp1jS2nJtTptwYIFpm/fviYuLs5ceeWVZvny5UHHjx8/bh544AFz8cUXm4suusjcfPPNVn+2z3+LxHo98cQTRtIZW2FhYWtMscW09Vqdftt3U1tlZWVrTbNFtPVaHTx40IwePdp4PB4THR1t0tLSzB133GH27t0b9hwcxhgT3r0ZAACAttFhX8MCAAA6DoIFAABYj2ABAADWI1gAAID1CBYAAGA9ggUAAFiPYAEAANYjWAAAgPUIFgAAYD2CBQAAWI9gAQAA1iNYAACA9f4f0ppSMkYWRr4AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9e3VAwouiQK"
      },
      "source": [
        "## **7. Crafting Adversarial Emails & Attacking SVM**\n",
        "After we find the magical words, we then add them to a spam email to evaluate how effectively the \"magic words\" can bypass the classifier. This is what we called \"crafting adversarial emails\". Then, we feed the new TF-IDF vectors of these crafted emails to the SVM classifier to see if they would be misclassified as ham emails.  \\\\\n",
        "\n",
        "\\\\\n",
        "**Run the code block below:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxY8tJakw2yv"
      },
      "source": [
        "m2_empty = pd.DataFrame()\n",
        "spam_cnt = 0\n",
        "threads = []\n",
        "m2_empty_l1 = pd.DataFrame()\n",
        "m2_empty_l2 = pd.DataFrame()\n",
        "m2_empty_l3 = pd.DataFrame()\n",
        "m2_empty_l4 = pd.DataFrame()\n",
        "m2_list = [m2_empty_l1, m2_empty_l2, m2_empty_l3, m2_empty_l4]\n",
        "\n",
        "\n",
        "def single_transform(x, method, feature_model, feature_names, scalar, selection_model):\n",
        "\n",
        "  result = feature_model.transform(x)\n",
        "\n",
        "  if selection_model != 'NaN':\n",
        "    result = selection_model.transform(result)\n",
        "\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "class myThread(threading.Thread):\n",
        "\n",
        "    def __init__(self, threadID, name, spam_message, words14str, method, feature_model, feature_names, scalar, clf_lin, list_index, selection_model):\n",
        "        threading.Thread.__init__(self)\n",
        "        self.threadID = threadID\n",
        "        self.name = name\n",
        "        self.spam_message = spam_message\n",
        "        self.words14str = words14str\n",
        "        self.method = method\n",
        "        self.feature_model = feature_model\n",
        "        self.feature_names = feature_names\n",
        "        self.scalar = scalar\n",
        "        self.clf_lin = clf_lin\n",
        "        self.list_index = list_index\n",
        "        self.lock = threading.Lock()\n",
        "        self.selection_model = selection_model\n",
        "\n",
        "    def run(self):\n",
        "        global spam_cnt\n",
        "        print(\"Starting \" + self.name)\n",
        "        spam_cnt_1 = m2_empty_out(self.name, self.spam_message, self.words14str, self.method,\n",
        "                                  self.feature_model, self.feature_names, self.scalar, self.clf_lin,\n",
        "                                  self.list_index, self.selection_model)\n",
        "        spam_cnt = spam_cnt+spam_cnt_1\n",
        "        time.sleep(0.1)\n",
        "        print(\"Exiting \" + self.name)\n",
        "\n",
        "\n",
        "\n",
        "def m2_empty_out(name, spam_message, words14str, method, feature_model, feature_names, scalar, clf_lin, list_index, selection_model):\n",
        "    m2_empty_1 = pd.DataFrame()\n",
        "    spam_cnt_1 = 0\n",
        "    global m2_list\n",
        "\n",
        "    for j in spam_message.message:\n",
        "        choose_email = [j + words14str]\n",
        "        message_14_email = pd.DataFrame(choose_email, columns=[\"message\"])\n",
        "        message_14_tf_idf = single_transform(\n",
        "            message_14_email[\"message\"], method, feature_model, feature_names, scalar, selection_model)\n",
        "        message_14_tf_idf = pd.DataFrame(\n",
        "            message_14_tf_idf.toarray(), columns=feature_names)\n",
        "        message_14_y = [1]\n",
        "        message_14_y = pd.Series(message_14_y)\n",
        "        message_CData = CDataset(message_14_tf_idf, message_14_y)\n",
        "        message_14_pred = clf_lin.predict(message_CData.X)\n",
        "\n",
        "        if message_14_pred == 0:\n",
        "            spam_cnt_1 = spam_cnt_1 + 1\n",
        "            m2_empty_1 = m2_empty_1.append(\n",
        "                message_14_tf_idf, ignore_index=True)\n",
        "\n",
        "    m2_list[list_index] = m2_list[list_index].append(\n",
        "        m2_empty_1, ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "    return spam_cnt_1\n",
        "\n",
        "\n",
        "\n",
        "def svm_attack(method, clf_lin, spam, words14str, feature_model, feature_names, scalar, selection_model):\n",
        "\n",
        "    global m2_empty\n",
        "\n",
        "    spam_messages = np.array_split(spam, 4)\n",
        "\n",
        "    print(\"Start processing message\")\n",
        "\n",
        "    thread1 = myThread(1, \"Thread-1\", spam_messages[0], words14str,\n",
        "                       method, feature_model, feature_names, scalar, clf_lin, 0, selection_model)\n",
        "    thread2 = myThread(2, \"Thread-2\", spam_messages[1], words14str,\n",
        "                       method, feature_model, feature_names, scalar, clf_lin, 1, selection_model)\n",
        "    thread3 = myThread(3, \"Thread-3\", spam_messages[2], words14str,\n",
        "                       method, feature_model, feature_names, scalar, clf_lin, 2, selection_model)\n",
        "    thread4 = myThread(4, \"Thread-4\", spam_messages[3], words14str,\n",
        "                       method, feature_model, feature_names, scalar, clf_lin, 3, selection_model)\n",
        "\n",
        "    threads.append(thread1)\n",
        "    threads.append(thread2)\n",
        "    threads.append(thread3)\n",
        "    threads.append(thread4)\n",
        "\n",
        "    for t in threads:\n",
        "        t.start()\n",
        "\n",
        "    for t in threads:\n",
        "        t.join()\n",
        "\n",
        "    m2_empty = m2_empty.append(m2_list[0], ignore_index=True)\n",
        "    m2_empty = m2_empty.append(m2_list[1], ignore_index=True)\n",
        "    m2_empty = m2_empty.append(m2_list[2], ignore_index=True)\n",
        "    m2_empty = m2_empty.append(m2_list[3], ignore_index=True)\n",
        "\n",
        "    print(\"Exiting Main Thread\")\n",
        "    print('White box attack on SVM:')\n",
        "    print('Number of samples provided:', len(spam))\n",
        "    print('Number of crafted sample that got misclassified:', spam_cnt)\n",
        "    print('Successful rate:', spam_cnt / len(spam))\n",
        "\n",
        "\n",
        "\n",
        "    return m2_empty"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raxGc9-UsT6d"
      },
      "source": [
        "**Run the code block below:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yo5YVW5zw0Iu",
        "outputId": "60295029-b060-4434-c851-6c074040bfe3"
      },
      "source": [
        "m2_empty = svm_attack('TFIDF', clf_lin, spam_test, words14str, feature_model, feature_names, scalar, 'NaN')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start processing message\n",
            "Starting Thread-1\n",
            "Starting Thread-2\n",
            "Starting Thread-3Starting Thread-4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_list[list_index] = m2_list[list_index].append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_list[list_index] = m2_list[list_index].append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exiting Thread-1\n",
            "Exiting Thread-2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_list[list_index] = m2_list[list_index].append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty_1 = m2_empty_1.append(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:111: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty = m2_empty.append(m2_list[0], ignore_index=True)\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:112: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty = m2_empty.append(m2_list[1], ignore_index=True)\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:113: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty = m2_empty.append(m2_list[2], ignore_index=True)\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-22-e9b4594de2c6>:114: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  m2_empty = m2_empty.append(m2_list[3], ignore_index=True)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exiting Thread-4\n",
            "Exiting Thread-3\n",
            "Exiting Main Thread\n",
            "White box attack on SVM:\n",
            "Number of samples provided: 96\n",
            "Number of crafted sample that got misclassified: 46\n",
            "Successful rate: 0.4791666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQOQYRvy4QBG"
      },
      "source": [
        "### **Question 6**\n",
        "Is the successful rate shown above higher or lower than the false negative rate of the classifer? Does that mean our attack is effective?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX--S66z4SF8"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znZP1evnzPPd"
      },
      "source": [
        "### **Task 4**\n",
        "Now we will try to craft individual emails by manually changing the text.\n",
        "Follow the steps below for 5 emails: \\\\\n",
        "\n",
        "1. Choose an email from the spam dataset.\n",
        "2. Copy the selected email into the code block.\n",
        "3. Add a single magical word obtained from the results of Section 6 to the email, placing it where you see fit.\n",
        "4. Run the code to check if the label is flipped from spam (1) to non-spam (0).\n",
        "5. If the label is flipped, stop. If not, repeat steps 3 and 4 (*add additional magic word*) until the label is flipped or until you've run out of magical words.\n",
        "\n",
        "Tips:\n",
        "1. Here you should note, what exactly is \"Spam Emails\" in this case.\n",
        "2. What should the magical words added to spam_emails be generated from? Is it our validation data or test data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XTYSvT12j5y",
        "outputId": "dccb7df5-49d1-4511-80dc-bfcc440d9f2b"
      },
      "source": [
        "# All Spam Emails\n",
        "print(spam_test)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                message  label\n",
            "159   abc super requalifier associated benefit consu...      1\n",
            "139   remove instruction remove request respectfully...      1\n",
            "2667  earn year sending email dear friend earn day s...      1\n",
            "2454  just released introducing million vol took tot...      1\n",
            "1745  know business need cash loan fast funding want...      1\n",
            "...                                                 ...    ...\n",
            "1286  new http capitalfm com help london child nt mi...      1\n",
            "1489  thank training course success course offer ext...      1\n",
            "2841  ll want live cybersex hour day right computer ...      1\n",
            "212   locate usa old friend lost loved one deadbeat ...      1\n",
            "474   america s largest monopoly shattered new growt...      1\n",
            "\n",
            "[96 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fqlpi6q3P4-",
        "outputId": "c8a5712a-2045-4aa5-f5ab-05705dca3e7c"
      },
      "source": [
        "# Choose by varying numbers\n",
        "print(spam_test.message.values[0])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abc super requalifier associated benefit consultant dear fellow entrepreneur letter urgent attempt reach like hear hr hopefully reached opportune time immediate reply attention needed today new income start little day want hear firsthand exciting breakthrough moneymaking opportunity abc super qualifier fast simple powerful moneymaker hoped discover research shown person s potential success increase receiving personalized instruction invite involved easy ingenious program involves personal instruction handpicked organization member professional skill expertise personally guide financial freedom immediately utilize existing program accomplishes following generates prequalified prospect send team support live interactive conference call cpm long distance virtual office stock ship product directly customer door pay daily overnight airborne express door permit realistic opportunity earn month produce new income spare time time time basis know wealth attained select advantage new powerful opportunity opportunity abc super qualifier time financial freedom earning income program offer reason instructed organization devote time genuine improving financial situation seriously looking real program produce large revenue urge office hr toll free number leave message notified response promptly return information need way making additional income week remember timing success john cheryl stephanie richard judy debra abc marketing team program try copy highly recommend abc paul barron ca ve seen fast simple way make large income alan louis tx abc perfect homebased business money great andrew kerr ca real program fast money thanks abc s herman tx gave mlm abc believer week c s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qygv01a_zOGe",
        "outputId": "e47a2672-7d81-4f65-b196-3d2378956a5f"
      },
      "source": [
        "choose_email = ['The email content you chose + magical word']\n",
        "message_14_email = pd.DataFrame(choose_email, columns=[\"message\"])\n",
        "message_14_tf_idf = single_transform(message_14_email[\"message\"], 'TFIDF',\n",
        "                    feature_model, feature_names, scalar,\n",
        "                    'NaN')\n",
        "message_14_tf_idf = pd.DataFrame(message_14_tf_idf.toarray(), columns=feature_names)\n",
        "message_14_y = [1]\n",
        "message_14_y = pd.Series(message_14_y)\n",
        "message_CData = CDataset(message_14_tf_idf, message_14_y)\n",
        "message_14_pred = clf_lin.predict(message_CData.X)\n",
        "\n",
        "# if the output is something like CArray([0]), then you flipped the label\n",
        "print('the label after injecting magical word:', message_14_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the label after injecting magical word: CArray([0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqgxp_kT4KgP"
      },
      "source": [
        "### **Question 7**\n",
        "How many emails successfully went through (classifiction label flipped to 0) after adding magical words? For each of these emails, how many magical words did you add? (Choose at least 5 emails)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrzWj7MU5Lwu"
      },
      "source": []
    }
  ]
}